{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Artificial Intelligence Nanodegree\n",
    "## Machine Translation Project\n",
    "In this notebook, sections that end with **'(IMPLEMENTATION)'** in the header indicate that the following blocks of code will require additional functionality which you must provide. Please be sure to read the instructions carefully!\n",
    "\n",
    "## Introduction\n",
    "In this notebook, you will build a deep neural network that functions as part of an end-to-end machine translation pipeline. Your completed pipeline will accept English text as input and return the French translation.\n",
    "\n",
    "- **Preprocess** - You'll convert text to sequence of integers.\n",
    "- **Models** Create models which accepts a sequence of integers as input and returns a probability distribution over possible translations. After learning about the basic types of neural networks that are often used for machine translation, you will engage in your own investigations, to design your own model!\n",
    "- **Prediction** Run the model on English text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%aimport helper, tests\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "import project_tests as tests\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import SimpleRNN, LSTM, GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify access to the GPU\n",
    "The following test applies only if you expect to be using a GPU, e.g., while running in a Udacity Workspace or using an AWS instance with GPU support. Run the next cell, and verify that the device_type is \"GPU\".\n",
    "- If the device is not GPU & you are running from a Udacity Workspace, then save your workspace with the icon at the top, then click \"enable\" at the bottom of the workspace.\n",
    "- If the device is not GPU & you are running from an AWS instance, then refer to the cloud computing instructions in the classroom to verify your setup steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5647755454578698350\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We begin by investigating the dataset that will be used to train and evaluate your pipeline.  The most common datasets used for machine translation are from [WMT](http://www.statmt.org/).  However, that will take a long time to train a neural network on.  We'll be using a dataset we created for this project that contains a small vocabulary.  You'll be able to train your model in a reasonable time with this dataset.\n",
    "### Load Data\n",
    "The data is located in `data/small_vocab_en` and `data/small_vocab_fr`. The `small_vocab_en` file contains English sentences with their French translations in the `small_vocab_fr` file. Load the English and French data from these files from running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "# Load English data\n",
    "english_sentences = helper.load_data('data/small_vocab_en')\n",
    "# Load French data\n",
    "french_sentences = helper.load_data('data/small_vocab_fr')\n",
    "\n",
    "print('Dataset Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files\n",
    "Each line in `small_vocab_en` contains an English sentence with the respective translation in each line of `small_vocab_fr`.  View the first two lines from each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_vocab_en Line 1:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "small_vocab_fr Line 1:  new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "small_vocab_en Line 2:  the united states is usually chilly during july , and it is usually freezing in november .\n",
      "small_vocab_fr Line 2:  les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n"
     ]
    }
   ],
   "source": [
    "for sample_i in range(2):\n",
    "    print('small_vocab_en Line {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n",
    "    print('small_vocab_fr Line {}:  {}'.format(sample_i + 1, french_sentences[sample_i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at the sentences, you can see they have been preprocessed already.  The puncuations have been delimited using spaces. All the text have been converted to lowercase.  This should save you some time, but the text requires more preprocessing.\n",
    "### Vocabulary\n",
    "The complexity of the problem is determined by the complexity of the vocabulary.  A more complex vocabulary is a more complex problem.  Let's look at the complexity of the dataset we'll be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1823250 English words.\n",
      "227 unique English words.\n",
      "10 Most common words in the English dataset:\n",
      "\"is\" \",\" \".\" \"in\" \"it\" \"during\" \"the\" \"but\" \"and\" \"sometimes\"\n",
      "\n",
      "1961295 French words.\n",
      "355 unique French words.\n",
      "10 Most common words in the French dataset:\n",
      "\"est\" \".\" \",\" \"en\" \"il\" \"les\" \"mais\" \"et\" \"la\" \"parfois\"\n"
     ]
    }
   ],
   "source": [
    "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
    "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
    "\n",
    "print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
    "print('{} unique English words.'.format(len(english_words_counter)))\n",
    "print('10 Most common words in the English dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
    "print()\n",
    "print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n",
    "print('{} unique French words.'.format(len(french_words_counter)))\n",
    "print('10 Most common words in the French dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, _Alice's Adventures in Wonderland_ contains 2,766 unique words of a total of 15,500 words.\n",
    "## Preprocess\n",
    "For this project, you won't use text data as input to your model. Instead, you'll convert the text into sequences of integers using the following preprocess methods:\n",
    "1. Tokenize the words into ids\n",
    "2. Add padding to make all the sequences the same length.\n",
    "\n",
    "Time to start preprocessing the data...\n",
    "### Tokenize (IMPLEMENTATION)\n",
    "For a neural network to predict on text data, it first has to be turned into data it can understand. Text data like \"dog\" is a sequence of ASCII character encodings.  Since a neural network is a series of multiplication and addition operations, the input data needs to be number(s).\n",
    "\n",
    "We can turn each character into a number or each word into a number.  These are called character and word ids, respectively.  Character ids are used for character level models that generate text predictions for each character.  A word level model uses word ids that generate text predictions for each word.  Word level models tend to learn better, since they are lower in complexity, so we'll use those.\n",
    "\n",
    "Turn each sentence into a sequence of words ids using Keras's [`Tokenizer`](https://keras.io/preprocessing/text/#tokenizer) function. Use this function to tokenize `english_sentences` and `french_sentences` in the cell below.\n",
    "\n",
    "Running the cell will run `tokenize` on sample data and show output for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'quick': 2, 'a': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'by': 10, 'jove': 11, 'my': 12, 'study': 13, 'of': 14, 'lexicography': 15, 'won': 16, 'prize': 17, 'this': 18, 'is': 19, 'short': 20, 'sentence': 21}\n",
      "\n",
      "Sequence 1 in x\n",
      "  Input:  The quick brown fox jumps over the lazy dog .\n",
      "  Output: [1, 2, 4, 5, 6, 7, 1, 8, 9]\n",
      "Sequence 2 in x\n",
      "  Input:  By Jove , my quick study of lexicography won a prize .\n",
      "  Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n",
      "Sequence 3 in x\n",
      "  Input:  This is a short sentence .\n",
      "  Output: [18, 19, 3, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(x):\n",
    "    \"\"\"\n",
    "    Tokenize x\n",
    "    :param x: List of sentences/strings to be tokenized\n",
    "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    tokenizer = Tokenizer(num_words=None, lower=True, char_level=False)\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    return tokenizer.texts_to_sequences(x), tokenizer\n",
    "\n",
    "tests.test_tokenize(tokenize)\n",
    "\n",
    "# Tokenize Example output\n",
    "text_sentences = [\n",
    "    'The quick brown fox jumps over the lazy dog .',\n",
    "    'By Jove , my quick study of lexicography won a prize .',\n",
    "    'This is a short sentence .']\n",
    "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "print(text_tokenizer.word_index)\n",
    "print()\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(sent))\n",
    "    print('  Output: {}'.format(token_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding (IMPLEMENTATION)\n",
    "When batching the sequence of word ids together, each sequence needs to be the same length.  Since sentences are dynamic in length, we can add padding to the end of the sequences to make them the same length.\n",
    "\n",
    "Make sure all the English sequences have the same length and all the French sequences have the same length by adding padding to the **end** of each sequence using Keras's [`pad_sequences`](https://keras.io/preprocessing/sequence/#pad_sequences) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1 in x\n",
      "  Input:  [1 2 4 5 6 7 1 8 9]\n",
      "  Output: [1 2 4 5 6]\n",
      "Sequence 2 in x\n",
      "  Input:  [10 11 12  2 13 14 15 16  3 17]\n",
      "  Output: [10 11 12  2 13]\n",
      "Sequence 3 in x\n",
      "  Input:  [18 19  3 20 21]\n",
      "  Output: [18 19  3 20 21]\n"
     ]
    }
   ],
   "source": [
    "def pad(x, length=None):\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    max_words = length\n",
    "    if max_words is None:\n",
    "        max_words = len(max(x))\n",
    "        \n",
    "    return pad_sequences(x, maxlen=max_words, dtype='int32', truncating='post', padding='post', value=0)\n",
    "\n",
    "\n",
    "tests.test_pad(pad)\n",
    "\n",
    "# Pad Tokenized output\n",
    "test_pad = pad(text_tokenized)\n",
    "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(np.array(token_sent)))\n",
    "    print('  Output: {}'.format(pad_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Pipeline\n",
    "Your focus for this project is to build neural network architecture, so we won't ask you to create a preprocess pipeline.  Instead, we've provided you with the implementation of the `preprocess` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n",
      "Max English sentence length: 5\n",
      "Max French sentence length: 5\n",
      "English vocabulary size: 199\n",
      "French vocabulary size: 344\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x, y):\n",
    "    \"\"\"\n",
    "    Preprocess x and y\n",
    "    :param x: Feature List of sentences\n",
    "    :param y: Label List of sentences\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
    "    \"\"\"\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
    "    preprocess(english_sentences, french_sentences)\n",
    "    \n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "french_vocab_size = len(french_tokenizer.word_index)\n",
    "\n",
    "print('Data Preprocessed')\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max French sentence length:\", max_french_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"French vocabulary size:\", french_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "In this section, you will experiment with various neural network architectures.\n",
    "You will begin by training four relatively simple architectures.\n",
    "- Model 1 is a simple RNN\n",
    "- Model 2 is a RNN with Embedding\n",
    "- Model 3 is a Bidirectional RNN\n",
    "- Model 4 is an optional Encoder-Decoder RNN\n",
    "\n",
    "After experimenting with the four simple architectures, you will construct a deeper architecture that is designed to outperform all four models.\n",
    "### Ids Back to Text\n",
    "The neural network will be translating the input to words ids, which isn't the final form we want.  We want the French translation.  The function `logits_to_text` will bridge the gab between the logits from the neural network to the French translation.  You'll be using this function to better understand the output of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: Logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "print('`logits_to_text` function loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: RNN (IMPLEMENTATION)\n",
    "![RNN](images/rnn.png)\n",
    "A basic RNN model is a good baseline for sequence data.  In this model, you'll build a RNN that translates English to French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344\n",
      "344\n",
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/10\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 3.3722 - acc: 0.2523"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Received a label value of 344 which is outside the valid range of [0, 344).  Label values: 7 88 1 64 16 11 30 1 67 15 62 1 92 2 41 7 33 1 9 7 29 1 65 37 11 10 38 1 20 16 36 1 12 67 37 27 145 7 97 134 3 113 115 184 110 3 94 4 28 4 11 30 1 19 15 3 55 4 72 4 3 55 4 78 4 3 55 4 28 4 11 30 1 12 57 4 32 31 1 8 10 82 1 61 16 10 85 1 20 16 10 38 1 40 16 62 1 12 63 2 99 124 4 70 4 7 89 1 39 16 10 85 1 39 13 36 1 8 18 2 20 13 16 17 1 20 16 10 23 14 35 34 1 8 19 4 32 31 1 9 36 1 8 57 2 3 94 4 71 4 64 16 17 1 10 29 1 19 2 52 29 1 8 66 2 29 1 12 21 37 29 1 8 66 15 4 32 31 1 21 7 36 1 8 65 33 1 57 2 49 7 33 1 8 63 36 1 69 21 2 7 33 1 12 65 20 16 17 1 13 39 109 17 1 13 7 88 1 7 13 64 16 17 13 1 62 1 21 2 50 35 34 1 66 2 4 32 31 105 131 27 94 7 148 97 29 1 58 69 2 99 127 91 28 76 4 32 31 1 12 27 55 4 75 4 4 32 31 1 8 62 1 12 65 37 11 30 1 12 18 101 1 12 18 15 29 1 19 2 41 7 33 1 9 18 62 105 103 9 19 135 28 76 4 28 3 55 4 73 4 7 90 1 61 16 3 180 35 34 24 27 94 4 78 4 36 105 103 9 19 11 30 1 19 2 3 55 10 163 110 11 30 1 19 15 35 34 1 58 8 7 86 1 39 16 84 1 39 16 13 4 32 31 1 8 36 1 12 18 2 11 30 1 19 15 29 1 12 57 2 7 86 1 61 16 35 34 1 12 18 107 126 4 77 28 29 1 65 2 52 11 30 1 12 67 20 16 1 13 14 7 62 1 9 58 11 30 1 58 2 4 32 31 1 18 107 132 10 38 4 35 34 1 12 68 27 55 4 75 10 40 109 17 1 146 20 16 1 13 14 3 55 4 72 4 11 30 1 10 81 4 32 31 1 12 62 1 12 63 2 62 1 8 104 2 7 83 1 20 16 29 1 8 19 2 40 13 14 108 96 7 88 1 61 16 7 62 1 58 8 11 30 1 9 58 7 194 161 10 189 7 33 1 12 67 7 33 1 12 18 29 1 8 104 15 7 83 1 40 16 115 221 1 40 109 35 34 1 104 2 29 1 7 102 15 3 118 113 10 200 29 1 8 60 2 35 34 1 12 59 61 16 17 1 13 4 32 31 1 63 29 1 9 57 2 29 1 8 19 2 27 55 4 70 4 29 1 12 67 2 3 55 4 28 4 61 16 17 1 84 39 16 17 1 7 7 80 1 40 16 20 16 1 13 14 29 1 10 81 37 10 82 1 39 16 7 33 1 9 18 10 85 1 40 16 29 1 66 15 25 99 124 4 28 6 20 16 10 23 14 3 55 4 78 6 29 1 8 59 2 7 36 1 12 57 99 127 91 4 77 29 1 8 67 2 7 33 1 8 58 36 1 9 66 37 10 38 1 20 16 35 34 1 9 58 27 94 115 110 172 20 16 1 13 14 7 36 1 8 60 39 16 1 13 14 29 1 58 69 2 4 32 31 1 8 35 34 1 12 18 7 33 1 18 15 4 32 31 1 58 36 1 60 2 54 27 94 4 77 4 4 32 31 1 12 29 1 9 60 15 36 1 8 19 2 3 55 4 73 4 107 208 22 136 7 7 36 1 8 63 20 16 10 23 14 11 30 1 66 37 7 33 1 12 68 11 30 1 26 63 7 86 1 20 16 29 1 9 60 15 61 13 14 16 1 7 83 1 61 16 7 33 1 9 18 62 1 9 7 102 36 1 104 2 24 40 109 10 23 156 20 16 17 1 13 7 87 1 64 16 3 1 198 22 164 84 1 20 16 17 11 30 1 58 2 11 30 1 9 18 129 96 10 23 14 27 1 15 230 79 61 16 17 1 7 4 32 31 1 19 27 94 4 71 4 27 55 10 82 10 3 55 4 28 4 35 34 1 9 18 10 85 1 40 16 27 94 4 72 4 7 33 1 8 19 4 32 31 1 12 4 32 31 1 9 3 113 7 168 97 7 36 1 8 104 36 1 12 92 2 35 34 105 95 2 27 94 4 28 4 11 30 1 9 18 20 16 17 1 7 3 203 123 2 62 36 1 21 2 54 3 100 4 74 4 7 33 1 8 58 7 83 1 39 16 27 55 4 70 4 35 34 1 18 2 36 1 19 2 43 10 85 1 61 16 170 323 141 131 108 35 34 1 12 65 36 1 19 37 11 7 90 1 39 16 20 16 10 23 14 10 82 1 39 16 101 1 9 59 2 27 55 4 71 4 3 118 149 244 182 11 30 1 12 63 10 82 1 61 16 7 90 1 13 20 29 1 8 66 37 35 34 1 8 19 27 100 120 148 97 29 1 10 81 69 62 1 12 66 2 107 126 28 76 4 191 96 13 14 1 11 30 1 69 21 35 34 1 58 2 135 4 70 4 72 7 33 1 9 60 7 33 1 8 19 4 32 31 1 21 7 80 1 39 13 33 1 21 15 25 29 1 195 26 98 29 1 12 60 2 4 32 31 1 59 20 16 10 23 14 35 34 1 12 60 27 55 4 74 28 3 145 115 163 110 29 1 12 67 2 62 1 8 57 37 35 34 1 8 66 29 1 9 58 15 29 1 9 57 37 39 16 17 1 7 62 1 18 2 49 40 16 17 1 10 11 30 1 8 63 3 55 91 7 147 29 1 8 21 2 11 30 1 9 58 99 124 10 38 4 99 127 4 74 4 29 1 12 18 2 35 34 1 9 66 29 1 9 67 37 27 55 4 73 4 20 16 1 13 14 10 38 1 20 16 99 124 4 72 4 20 16 1 13 14 3 55 91 10 38 101 1 9 18 2 10 85 1 61 16 20 109 10 23 14 29 1 21 2 45 27 145 121 148 97 3 55 4 78 4 3 55 4 75 4 101 1 58 69 2 11 30 1 12 59 7 36 1 10 81 3 55 4 78 4 36 1 19 37 11 11 30 1 9 125 191 96 13 14 1 11 30 1 8 7 3 100 4 72 10 7 80 1 20 16 4 32 31 1 9 27 55 4 74 10 39 13 14 1 10 11 30 1 8 65 27 143 121 151 97 3 158 91 7 147 146 10 218 1 40 10 220 1 20 109 29 1 12 19 15 7 33 1 9 18 101 1 9 65 15 35 34 1 9 66 62 1 8 60 2 7 33 1 12 21 10 82 1 20 16 1 3 223 15 35 4 32 31 1 9 20 16 17 1 7 11 30 1 59 15 3 158 91 144 163 7 86 1 64 16 35 34 1 19 15 11 30 1 104 15 20 16 10 23 14 62 105 103 9 19 3 113 120 168 97 64 16 17 1 10 35 34 1 9 66 33 1 66 37 11 4 32 31 1 12 29 1 9 59 2 29 1 8 60 15 35 34 1 12 59 36 1 9 7 102 7 33 1 8 58 62 1 8 57 2 35 34 1 9 18 62 1 8 104 15 107 126 10 38 4 36 1 9 59 37 7 33 1 10 81 35 34 1 60 2 7 83 1 10 16 84 1 20 16 17 99 127 91 28 76 7 36 1 8 10 29 1 8 10 81 4 32 31 1 9 101 1 8 18 2 7 33 1 9 60 99 127 91 4 77 39 16 10 23 14 7 33 1 60 26 11 30 1 104 26 29 1 59 26 98 3 55 4 73 4 36 1 19 2 52 29 1 12 59 2 7 89 1 20 16 10 85 1 20 16 29 1 57 15 25 106 130 4 74 6 101 1 8 18 37 4 32 31 1 9 20 16 1 13 14 99 127 10 38 4 99 127 4 75 28 40 16 17 13 1 29 1 8 18 37 4 32 31 1 12 36 1 104 2 45 116 100 4 78 4 3 94 4 71 4 3 55 91 4 77 11 30 1 243 15 33 105 103 9 19 7 89 1 39 13 7 80 1 20 13 4 32 31 1 19 35 34 1 58 8 4 32 31 1 9 35 34 1 9 18 3 55 10 38 28 4 32 31 1 9 3 118 113 7 97 64 16 1 13 14 4 32 31 1 9 3 55 91 28 76 11 30 1 12 92 27 143 7 151 97 62 1 9 63 2 35 34 1 9 57 20 16 17 1 13 36 1 12 18 2 11 30 1 12 19 212 99 124 278 0 7 87 1 20 16 3 94 4 70 4 135 4 74 4 73 11 30 1 9 19 27 94 4 28 4 29 1 9 26 63 7 90 1 20 16 36 1 9 125 2 7 90 1 64 16 7 33 1 104 2 27 55 7 288 153 11 30 1 9 21 20 16 17 1 10 11 30 1 104 2 7 62 1 10 81 3 55 4 70 6 27 113 120 148 97 106 130 28 76 4 20 16 10 23 14 4 32 31 1 9 27 55 4 77 4 29 1 69 21 2 116 100 4 74 4 29 1 9 66 15 129 96 10 23 14 29 1 12 66 15 146 7 201 1 39 4 32 31 1 9 27 55 4 74 4 29 1 8 112 15 3 160 171 1 198 27 55 4 77 78 3 100 4 75 28 11 30 1 9 59 106 130 28 76 4 4 32 31 1 8 11 30 1 7 102 27 94 10 38 4 116 100 38 4 71 20 16 1 13 14 36 1 8 63 37 84 1 20 16 10 7 90 1 61 16 29 1 68 2 52 27 55 4 70 4 11 30 1 18 2 29 1 9 19 2 36 105 103 9 19 20 16 10 23 14 135 4 73 4 28 29 1 9 58 2 35 34 1 8 21 29 1 8 63 2 36 1 63 2 52 29 1 68 15 25 7 80 1 64 16 7 33 1 12 59 116 207 22 136 7 64 16 17 1 10 3 55 115 248 110 35 34 1 9 117 35 34 1 19 2 27 94 4 75 6 3 94 121 147 97 36 1 8 18 2 29 1 9 138 2 99 131 233 2 36 11 30 1 8 59 101 1 9 65 2 35 34 105 103 9 29 1 9 18 26 61 16 17 13 1 3 100 4 74 4 4 32 31 1 12 62 1 8 92 2 27 55 4 78 4 135 4 71 6 4 7 33 1 12 63 7 33 1 8 10 29 1 9 22 102 20 16 1 13 14 11 30 1 12 68 129 96 13 14 1 135 10 38 4 77 3 55 4 28 4 35 34 1 8 21 11 30 1 12 68 99 124 10 38 6 29 1 9 58 37 29 1 8 19 2 7 62 1 9 10 36 1 58 26 98 40 16 10 23 14 27 143 146 165 97 7 86 1 61 16 11 30 1 8 21 115 216 122 20 109 107 126 4 77 4 107 126 4 78 4 3 143 10 248 110 11 30 1 10 81 35 34 1 9 65 27 1 10 344 22 62 1 9 19 2 10 82 1 64 16 35 34 1 10 81 27 178 22 167 181 4 32 31 1 21 27 55 4 71 4 3 55 91 10 38 20 16 17 1 10 7 33 1 9 58 263 179 209 141 175 4 32 31 1 7 7 33 1 12 18 62 1 69 21 15 36 1 9 117 2 4 32 31 1 8 64 16 17 1 84 20 16 17 13 1 99 124 4 72 4 35 34 1 21 2 7 36 1 8 306 4 32 31 1 21 40 16 1 13 14 35 34 1 12 60 7 33 1 9 18 35 34 1 12 63 33 1 57 2 43 29 1 12 59 2 20 16 10 23 14 135 4 71 4 72 39 16 17 1 7 7 33 1 9 19 11 30 1 8 18 7 89 1 20 16 62 1 12 63 2 35 34 1 63 37 64 16 17 1 13 29 1 9 125 2 29 1 8 18 2 27 145 120 148 97 146 11 219 122 20 99 124 4 71 4 35 34 1 7 102 29 1 8 112 37 3 94 4 71 10 106 130 4 73 4 27 55 28 76 4 4 32 31 1 12 33 1 21 2 54 27 158 91 144 183 3 55 4 70 4 3 94 4 70 4 29 1 9 66 2 11 30 1 12 60 3 118 113 144 200 99 124 4 78 4 40 16 17 13 1 62 1 8 104 26 20 16 17 1 7 27 158 91 7 151 36 1 8 66 2 7 33 1 10 81 39 16 17 1 7 4 32 31 1 9 101 1 8 18 15 7 33 1 8 63 107 132 4 71 6 11 30 1 9 22 27 55 121 148 97 4 32 31 1 12 10 85 1 64 16 35 34 1 9 65 7 33 1 9 19 35 34 1 18 2 27 55 120 97 155 4 32 31 1 12 4 32 31 1 8 29 1 12 92 2 4 32 31 1 12 61 16 17 1 10 4 32 31 1 9 263 179 209 141 4 7 33 1 8 58 36 1 8 137 2 106 128 91 4 28 40 16 10 23 14 62 1 8 67 2 7 33 1 8 92 4 32 31 1 9 29 1 8 18 15 7 36 1 8 92 11 30 1 12 18 11 30 1 19 2 106 130 4 73 4 235 1 115 290 180 35 34 1 10 81 36 1 9 10 81 61 16 1 13 14 62 1 9 7 102 101 1 9 195 37 40 13 14 1 10 29 1 12 67 15 35 34 1 8 67 20 16 1 13 14 3 55 4 70 4 7 33 1 9 104 27 55 121 151 97 7 33 1 12 68 62 1 8 60 2 7 33 1 9 104 4 32 31 1 10 10 38 1 64 16 106 130 4 73 4 129 96 13 14 1 4 32 31 1 8 106 130 4 28 4 84 1 13 39 16 27 204 123 15 35 27 118 149 121 97 7 62 1 10 81 20 16 1 13 14 7 201 1 20 109 29 1 21 15 25 35 34 1 60 15 11 30 1 63 15 3 55 91 4 73 40 16 10 23 14 29 1 8 18 15 10 85 1 20 16 4 32 31 1 8 11 30 1 9 60 61 13 14 16 1 106 128 91 4 78 7 90 1 20 16 36 1 9 138 2 64 16 17 1 13 7 86 1 20 16 4 32 31 1 58 29 1 8 21 15 35 34 1 12 63 29 1 9 67 15 101 1 8 67 2 20 16 17 13 1 61 16 17 1 7 35 34 1 8 63 35 34 1 9 58 3 113 121 168 97 11 30 1 8 59 3 100 4 75 4 7 87 1 64 16 7 80 1 40 16 36 1 7 102 15 29 1 12 18 2 29 1 12 63 37 29 1 60 15 111 62 1 8 19 15 116 160 171 1 196 4 32 31 1 8 36 1 8 67 2 27 55 4 71 4 36 1 9 22 102 36 1 12 67 2 3 55 146 11 193 3 55 91 28 76 7 90 1 61 16 29 1 8 19 2 7 33 1 8 18 4 32 31 1 12 170 141 131 108 258 107 126 4 28 4 107 132 4 28 4 39 16 10 23 14 36 1 60 15 25 3 113 121 151 97 4 32 31 1 10 29 1 9 104 37 29 1 58 69 15 4 32 31 1 9 62 1 8 92 2 101 1 19 37 11 20 109 10 23 14 7 86 1 64 16 11 30 1 9 67 7 33 1 8 60 7 87 1 10 16 39 16 17 1 13 3 55 10 38 4 4 32 31 1 9 35 34 1 10 81 99 124 4 71 4 7 89 1 20 16 62 1 57 2 48 7 33 1 9 65 62 105 103 9 19 99 124 4 70 4 40 109 10 23 14 7 36 1 12 59 27 55 4 70 10 35 34 1 12 18 4 32 31 1 12 106 130 4 73 6 62 1 12 19 2 3 55 4 73 4 3 100 4 75 28 101 1 60 15 25 135 4 74 4 78 33 1 59 15 25 29 1 65 37 11 33 105 103 9 19 29 1 9 65 37 107 208 120 254 2 7 86 1 20 16 152 96 10 23 14 152 96 10 23 14 39 16 1 13 14 11 30 1 8 18 10 38 1 20 13 40 16 10 23 14 107 132 4 78 4 7 36 1 8 57 27 118 113 121 168 27 118 113 121 168 62 1 12 57 2 11 30 1 69 65 29 1 8 19 15 3 55 91 4 75 7 33 1 8 58 11 30 1 249 15 35 34 1 12 19 7 88 1 61 16 29 1 66 2 49 99 127 4 73 4 33 1 58 37 11 27 158 91 7 151 7 90 1 20 16 205 229 99 10 277 20 16 17 1 7 7 33 1 12 63 62 1 9 19 2 10 85 1 39 16 7 33 1 12 67 7 80 1 39 16 10 85 1 61 16 7 90 1 13 39 29 1 19 2 49 20 16 17 13 1 40 16 10 23 14 29 1 12 19 2 84 1 13 39 16 27 55 4 28 6 29 1 8 66 15 3 143 120 97 134 36 1 8 63 37 116 100 91 4 71 62 1 12 18 2 7 33 1 104 15 62 1 8 59 2 7 33 1 8 104 35 34 1 8 18 7 33 1 12 92 106 130 4 74 6 4 32 31 1 8 4 32 31 1 112 27 94 4 73 4 29 1 8 18 2 99 240 123 2 36 39 16 17 1 84 7 62 1 8 10 29 1 12 68 15 101 1 58 69 26 20 109 10 23 156 27 94 115 110 182 7 62 1 9 58 35 34 1 9 10 7 87 1 61 16 35 34 1 10 81 62 1 63 15 25 135 4 73 4 159 11 30 1 10 81 7 87 1 7 13 116 100 91 4 75 27 55 4 77 4 106 128 91 4 77 7 33 1 8 18 35 34 1 8 66 7 86 1 61 16 20 16 17 1 13 4 32 31 1 8 62 1 8 21 2 3 158 91 144 110 3 178 22 136 29 10 82 1 40 16 40 16 17 1 13 62 1 9 104 26 29 1 12 63 2 152 96 10 23 14 39 109 17 1 13 7 87 1 10 16 4 32 31 1 8 11 30 1 12 19 62 1 66 2 50 4 32 31 1 67 27 55 10 184 110 7 33 1 12 67 7 87 1 40 13 20 16 1 13 14 3 94 4 28 4 27 1 234 2 48 7 33 1 8 59 35 34 1 10 81 11 30 1 12 60 11 30 1 9 26 61 16 17 1 7 152 96 10 23 14 101 1 12 59 2 29 1 66 15 42 101 1 9 60 2 7 33 1 12 59 29 1 21 26 98 7 33 1 8 18 29 1 9 22 102 64 16 17 1 7 27 55 4 77 4 3 55 91 6 28 35 34 1 9 60 29 1 12 65 2 4 32 31 1 65 7 33 1 12 19 3 158 91 121 97 4 32 31 1 9 10 85 1 39 16 20 109 10 23 14 4 32 31 1 9 3 94 4 28 28 64 16 17 1 7 10 82 1 20 16 7 62 1 9 10 106 130 4 74 4 29 1 8 19 2 29 1 8 21 15 4 32 31 1 18 135 4 74 4 71 35 34 1 8 66 20 16 10 23 14 7 33 1 8 21 33 1 59 2 51 29 1 10 81 69 35 34 1 18 26 33 1 63 2 48 10 85 1 61 16 99 127 38 28 76 29 1 67 37 11 29 1 8 63 37 7 36 1 58 69 152 96 10 23 14 27 55 4 73 4 29 1 9 59 26 40 16 17 1 7 29 1 9 65 2 64 16 17 1 13 4 32 31 1 19 3 160 171 1 198 11 30 1 10 81 7 33 1 9 68 235 294 116 26 29 7 33 1 12 68 27 160 146 303 197 99 124 4 77 4 29 1 8 92 2 62 1 65 2 51 11 30 1 9 104 101 1 9 59 37 7 36 1 10 81 61 13 14 16 1 11 30 1 9 60 11 30 1 8 137 36 1 9 58 37 10 38 1 64 16 33 1 67 15 42 10 38 1 20 16 27 55 10 38 4 27 145 120 151 97 3 55 4 70 4 11 30 1 8 18 129 96 10 23 14 61 16 17 1 13 36 1 21 15 25 29 1 8 92 2 20 16 17 13 1 27 55 4 70 4 35 34 1 9 117 36 1 21 15 25 7 33 1 9 58 40 16 17 1 10 4 32 31 1 9 152 96 10 23 14 27 94 4 70 6 35 34 1 12 67 7 86 1 20 13 11 30 1 8 57 4 257 131 170 141 7 90 1 39 13 10 85 1 7 13 4 32 31 1 58 7 33 1 12 18 116 160 146 7 194 99 127 4 71 28 115 217 122 40 109 27 94 10 110 133 36 1 9 10 81 20 16 17 1 7 10 82 1 39 16 29 1 8 18 2 29 1 12 65 2 20 16 17 1 7 61 16 17 1 7 36 1 8 67 2 7 33 1 60 2 29 1 9 10 81 35 34 1 9 63 27 55 120 147 97 3 100 4 72 4 4 32 31 1 249 11 30 1 249 15 7 80 1 20 16 36 1 9 22 225 29 1 63 37 11 20 16 17 1 13 11 30 1 69 65 29 1 8 57 2 27 94 4 28 28 3 55 4 28 4 11 30 1 9 67 4 32 31 1 12 62 1 8 63 2 27 55 91 28 76 35 34 1 57 2 4 32 31 105 131 99 124 4 28 4 3 55 91 120 168 62 1 8 68 2 27 94 4 28 38 27 94 4 78 4 29 1 9 21 15 35 34 1 104 2 7 87 1 40 16 39 13 14 108 96 27 55 7 168 97 20 16 1 13 14 101 1 9 60 2 35 34 1 12 57 4 32 31 1 12 7 89 1 64 16 106 130 10 38 6 27 180 173 32 31 205 131 99 91 123 35 34 1 8 65 7 36 1 8 19 101 1 60 15 25 29 1 8 65 37 33 1 66 37 11 29 1 8 18 2 20 16 17 13 1 255 122 40 254 26 101 1 8 67 15 29 1 12 19 15 11 30 1 9 66 7 33 1 8 63 27 143 120 148 97 36 1 9 66 15 20 16 17 13 1 27 94 4 71 4 116 100 91 4 77 7 33 1 8 92 3 94 4 71 4 35 34 1 12 18 35 34 1 19 15 4 32 31 1 58 3 143 115 184 110 106 130 4 70 4 11 30 1 9 117 62 1 63 37 11 3 94 4 28 4 106 128 91 4 72 4 32 31 1 12 84 1 64 16 17 7 80 1 20 16 11 30 1 9 7 7 86 1 64 16 27 55 4 71 4 3 55 91 10 200 4 32 31 1 12 36 1 9 60 2 10 82 1 40 16 10 82 1 39 13 29 1 12 18 15 106 130 10 38 4 11 30 1 58 2 99 124 4 73 10 7 83 1 7 13 35 34 1 9 63 29 1 8 18 2 11 30 1 12 57 20 16 10 23 14 35 34 1 8 18 7 80 1 39 13 7 33 1 9 104 11 30 1 8 18 3 143 121 147 97 3 100 4 74 4 11 30 1 12 19 7 86 1 61 16 61 16 17 1 7 36 1 8 18 2 7 33 1 8 137 3 55 4 74 4 4 32 31 1 8 3 145 120 97 134 7 36 1 58 12 35 34 1 8 19 27 94 4 28 4 7 89 1 20 16 39 16 10 23 14 38 131 191 96 4 27 247 121 147 97 4 32 31 1 8 27 94 4 75 4 62 1 21 2 49 36 1 8 66 2 35 34 1 12 92 39 16 17 1 7 11 30 1 9 104 27 55 4 74 4 4 32 31 1 60 36 1 9 22 102 27 55 4 77 6 3 55 4 78 4 3 55 4 77 4 7 36 1 8 10 11 30 1 67 2 35 34 1 9 58 129 96 10 23 14 4 32 31 1 58 20 16 17 13 1 106 128 91 4 78 84 1 40 16 13 3 100 4 73 4 3 1 198 22 164 4 32 31 1 9\n\t [[Node: loss_1/time_distributed_2_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss_1/time_distributed_2_loss/Reshape_1, loss_1/time_distributed_2_loss/Cast)]]\n\nCaused by op 'loss_1/time_distributed_2_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits', defined at:\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/asyncio/base_events.py\", line 1425, in _run_once\n    handle._run()\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/asyncio/events.py\", line 126, in _run\n    self._callback(*self._args)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-08006f21ec40>\", line 64, in <module>\n    french_vocab_size)\n  File \"<ipython-input-13-08006f21ec40>\", line 25, in simple_model\n    model.compile(loss=sparse_categorical_crossentropy, optimizer=Adam(learning_rate), metrics=['accuracy'])\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/keras/models.py\", line 824, in compile\n    **kwargs)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/keras/engine/training.py\", line 830, in compile\n    sample_weight, mask)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/keras/engine/training.py\", line 429, in weighted\n    score_array = fn(y_true, y_pred)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/keras/losses.py\", line 73, in sparse_categorical_crossentropy\n    return K.sparse_categorical_crossentropy(y_true, y_pred)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 3041, in sparse_categorical_crossentropy\n    logits=logits)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 2052, in sparse_softmax_cross_entropy_with_logits\n    precise_logits, labels, name=name)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 7451, in sparse_softmax_cross_entropy_with_logits\n    labels=labels, name=name)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3290, in create_op\n    op_def=op_def)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1654, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Received a label value of 344 which is outside the valid range of [0, 344).  Label values: 7 88 1 64 16 11 30 1 67 15 62 1 92 2 41 7 33 1 9 7 29 1 65 37 11 10 38 1 20 16 36 1 12 67 37 27 145 7 97 134 3 113 115 184 110 3 94 4 28 4 11 30 1 19 15 3 55 4 72 4 3 55 4 78 4 3 55 4 28 4 11 30 1 12 57 4 32 31 1 8 10 82 1 61 16 10 85 1 20 16 10 38 1 40 16 62 1 12 63 2 99 124 4 70 4 7 89 1 39 16 10 85 1 39 13 36 1 8 18 2 20 13 16 17 1 20 16 10 23 14 35 34 1 8 19 4 32 31 1 9 36 1 8 57 2 3 94 4 71 4 64 16 17 1 10 29 1 19 2 52 29 1 8 66 2 29 1 12 21 37 29 1 8 66 15 4 32 31 1 21 7 36 1 8 65 33 1 57 2 49 7 33 1 8 63 36 1 69 21 2 7 33 1 12 65 20 16 17 1 13 39 109 17 1 13 7 88 1 7 13 64 16 17 13 1 62 1 21 2 50 35 34 1 66 2 4 32 31 105 131 27 94 7 148 97 29 1 58 69 2 99 127 91 28 76 4 32 31 1 12 27 55 4 75 4 4 32 31 1 8 62 1 12 65 37 11 30 1 12 18 101 1 12 18 15 29 1 19 2 41 7 33 1 9 18 62 105 103 9 19 135 28 76 4 28 3 55 4 73 4 7 90 1 61 16 3 180 35 34 24 27 94 4 78 4 36 105 103 9 19 11 30 1 19 2 3 55 10 163 110 11 30 1 19 15 35 34 1 58 8 7 86 1 39 16 84 1 39 16 13 4 32 31 1 8 36 1 12 18 2 11 30 1 19 15 29 1 12 57 2 7 86 1 61 16 35 34 1 12 18 107 126 4 77 28 29 1 65 2 52 11 30 1 12 67 20 16 1 13 14 7 62 1 9 58 11 30 1 58 2 4 32 31 1 18 107 132 10 38 4 35 34 1 12 68 27 55 4 75 10 40 109 17 1 146 20 16 1 13 14 3 55 4 72 4 11 30 1 10 81 4 32 31 1 12 62 1 12 63 2 62 1 8 104 2 7 83 1 20 16 29 1 8 19 2 40 13 14 108 96 7 88 1 61 16 7 62 1 58 8 11 30 1 9 58 7 194 161 10 189 7 33 1 12 67 7 33 1 12 18 29 1 8 104 15 7 83 1 40 16 115 221 1 40 109 35 34 1 104 2 29 1 7 102 15 3 118 113 10 200 29 1 8 60 2 35 34 1 12 59 61 16 17 1 13 4 32 31 1 63 29 1 9 57 2 29 1 8 19 2 27 55 4 70 4 29 1 12 67 2 3 55 4 28 4 61 16 17 1 84 39 16 17 1 7 7 80 1 40 16 20 16 1 13 14 29 1 10 81 37 10 82 1 39 16 7 33 1 9 18 10 85 1 40 16 29 1 66 15 25 99 124 4 28 6 20 16 10 23 14 3 55 4 78 6 29 1 8 59 2 7 36 1 12 57 99 127 91 4 77 29 1 8 67 2 7 33 1 8 58 36 1 9 66 37 10 38 1 20 16 35 34 1 9 58 27 94 115 110 172 20 16 1 13 14 7 36 1 8 60 39 16 1 13 14 29 1 58 69 2 4 32 31 1 8 35 34 1 12 18 7 33 1 18 15 4 32 31 1 58 36 1 60 2 54 27 94 4 77 4 4 32 31 1 12 29 1 9 60 15 36 1 8 19 2 3 55 4 73 4 107 208 22 136 7 7 36 1 8 63 20 16 10 23 14 11 30 1 66 37 7 33 1 12 68 11 30 1 26 63 7 86 1 20 16 29 1 9 60 15 61 13 14 16 1 7 83 1 61 16 7 33 1 9 18 62 1 9 7 102 36 1 104 2 24 40 109 10 23 156 20 16 17 1 13 7 87 1 64 16 3 1 198 22 164 84 1 20 16 17 11 30 1 58 2 11 30 1 9 18 129 96 10 23 14 27 1 15 230 79 61 16 17 1 7 4 32 31 1 19 27 94 4 71 4 27 55 10 82 10 3 55 4 28 4 35 34 1 9 18 10 85 1 40 16 27 94 4 72 4 7 33 1 8 19 4 32 31 1 12 4 32 31 1 9 3 113 7 168 97 7 36 1 8 104 36 1 12 92 2 35 34 105 95 2 27 94 4 28 4 11 30 1 9 18 20 16 17 1 7 3 203 123 2 62 36 1 21 2 54 3 100 4 74 4 7 33 1 8 58 7 83 1 39 16 27 55 4 70 4 35 34 1 18 2 36 1 19 2 43 10 85 1 61 16 170 323 141 131 108 35 34 1 12 65 36 1 19 37 11 7 90 1 39 16 20 16 10 23 14 10 82 1 39 16 101 1 9 59 2 27 55 4 71 4 3 118 149 244 182 11 30 1 12 63 10 82 1 61 16 7 90 1 13 20 29 1 8 66 37 35 34 1 8 19 27 100 120 148 97 29 1 10 81 69 62 1 12 66 2 107 126 28 76 4 191 96 13 14 1 11 30 1 69 21 35 34 1 58 2 135 4 70 4 72 7 33 1 9 60 7 33 1 8 19 4 32 31 1 21 7 80 1 39 13 33 1 21 15 25 29 1 195 26 98 29 1 12 60 2 4 32 31 1 59 20 16 10 23 14 35 34 1 12 60 27 55 4 74 28 3 145 115 163 110 29 1 12 67 2 62 1 8 57 37 35 34 1 8 66 29 1 9 58 15 29 1 9 57 37 39 16 17 1 7 62 1 18 2 49 40 16 17 1 10 11 30 1 8 63 3 55 91 7 147 29 1 8 21 2 11 30 1 9 58 99 124 10 38 4 99 127 4 74 4 29 1 12 18 2 35 34 1 9 66 29 1 9 67 37 27 55 4 73 4 20 16 1 13 14 10 38 1 20 16 99 124 4 72 4 20 16 1 13 14 3 55 91 10 38 101 1 9 18 2 10 85 1 61 16 20 109 10 23 14 29 1 21 2 45 27 145 121 148 97 3 55 4 78 4 3 55 4 75 4 101 1 58 69 2 11 30 1 12 59 7 36 1 10 81 3 55 4 78 4 36 1 19 37 11 11 30 1 9 125 191 96 13 14 1 11 30 1 8 7 3 100 4 72 10 7 80 1 20 16 4 32 31 1 9 27 55 4 74 10 39 13 14 1 10 11 30 1 8 65 27 143 121 151 97 3 158 91 7 147 146 10 218 1 40 10 220 1 20 109 29 1 12 19 15 7 33 1 9 18 101 1 9 65 15 35 34 1 9 66 62 1 8 60 2 7 33 1 12 21 10 82 1 20 16 1 3 223 15 35 4 32 31 1 9 20 16 17 1 7 11 30 1 59 15 3 158 91 144 163 7 86 1 64 16 35 34 1 19 15 11 30 1 104 15 20 16 10 23 14 62 105 103 9 19 3 113 120 168 97 64 16 17 1 10 35 34 1 9 66 33 1 66 37 11 4 32 31 1 12 29 1 9 59 2 29 1 8 60 15 35 34 1 12 59 36 1 9 7 102 7 33 1 8 58 62 1 8 57 2 35 34 1 9 18 62 1 8 104 15 107 126 10 38 4 36 1 9 59 37 7 33 1 10 81 35 34 1 60 2 7 83 1 10 16 84 1 20 16 17 99 127 91 28 76 7 36 1 8 10 29 1 8 10 81 4 32 31 1 9 101 1 8 18 2 7 33 1 9 60 99 127 91 4 77 39 16 10 23 14 7 33 1 60 26 11 30 1 104 26 29 1 59 26 98 3 55 4 73 4 36 1 19 2 52 29 1 12 59 2 7 89 1 20 16 10 85 1 20 16 29 1 57 15 25 106 130 4 74 6 101 1 8 18 37 4 32 31 1 9 20 16 1 13 14 99 127 10 38 4 99 127 4 75 28 40 16 17 13 1 29 1 8 18 37 4 32 31 1 12 36 1 104 2 45 116 100 4 78 4 3 94 4 71 4 3 55 91 4 77 11 30 1 243 15 33 105 103 9 19 7 89 1 39 13 7 80 1 20 13 4 32 31 1 19 35 34 1 58 8 4 32 31 1 9 35 34 1 9 18 3 55 10 38 28 4 32 31 1 9 3 118 113 7 97 64 16 1 13 14 4 32 31 1 9 3 55 91 28 76 11 30 1 12 92 27 143 7 151 97 62 1 9 63 2 35 34 1 9 57 20 16 17 1 13 36 1 12 18 2 11 30 1 12 19 212 99 124 278 0 7 87 1 20 16 3 94 4 70 4 135 4 74 4 73 11 30 1 9 19 27 94 4 28 4 29 1 9 26 63 7 90 1 20 16 36 1 9 125 2 7 90 1 64 16 7 33 1 104 2 27 55 7 288 153 11 30 1 9 21 20 16 17 1 10 11 30 1 104 2 7 62 1 10 81 3 55 4 70 6 27 113 120 148 97 106 130 28 76 4 20 16 10 23 14 4 32 31 1 9 27 55 4 77 4 29 1 69 21 2 116 100 4 74 4 29 1 9 66 15 129 96 10 23 14 29 1 12 66 15 146 7 201 1 39 4 32 31 1 9 27 55 4 74 4 29 1 8 112 15 3 160 171 1 198 27 55 4 77 78 3 100 4 75 28 11 30 1 9 59 106 130 28 76 4 4 32 31 1 8 11 30 1 7 102 27 94 10 38 4 116 100 38 4 71 20 16 1 13 14 36 1 8 63 37 84 1 20 16 10 7 90 1 61 16 29 1 68 2 52 27 55 4 70 4 11 30 1 18 2 29 1 9 19 2 36 105 103 9 19 20 16 10 23 14 135 4 73 4 28 29 1 9 58 2 35 34 1 8 21 29 1 8 63 2 36 1 63 2 52 29 1 68 15 25 7 80 1 64 16 7 33 1 12 59 116 207 22 136 7 64 16 17 1 10 3 55 115 248 110 35 34 1 9 117 35 34 1 19 2 27 94 4 75 6 3 94 121 147 97 36 1 8 18 2 29 1 9 138 2 99 131 233 2 36 11 30 1 8 59 101 1 9 65 2 35 34 105 103 9 29 1 9 18 26 61 16 17 13 1 3 100 4 74 4 4 32 31 1 12 62 1 8 92 2 27 55 4 78 4 135 4 71 6 4 7 33 1 12 63 7 33 1 8 10 29 1 9 22 102 20 16 1 13 14 11 30 1 12 68 129 96 13 14 1 135 10 38 4 77 3 55 4 28 4 35 34 1 8 21 11 30 1 12 68 99 124 10 38 6 29 1 9 58 37 29 1 8 19 2 7 62 1 9 10 36 1 58 26 98 40 16 10 23 14 27 143 146 165 97 7 86 1 61 16 11 30 1 8 21 115 216 122 20 109 107 126 4 77 4 107 126 4 78 4 3 143 10 248 110 11 30 1 10 81 35 34 1 9 65 27 1 10 344 22 62 1 9 19 2 10 82 1 64 16 35 34 1 10 81 27 178 22 167 181 4 32 31 1 21 27 55 4 71 4 3 55 91 10 38 20 16 17 1 10 7 33 1 9 58 263 179 209 141 175 4 32 31 1 7 7 33 1 12 18 62 1 69 21 15 36 1 9 117 2 4 32 31 1 8 64 16 17 1 84 20 16 17 13 1 99 124 4 72 4 35 34 1 21 2 7 36 1 8 306 4 32 31 1 21 40 16 1 13 14 35 34 1 12 60 7 33 1 9 18 35 34 1 12 63 33 1 57 2 43 29 1 12 59 2 20 16 10 23 14 135 4 71 4 72 39 16 17 1 7 7 33 1 9 19 11 30 1 8 18 7 89 1 20 16 62 1 12 63 2 35 34 1 63 37 64 16 17 1 13 29 1 9 125 2 29 1 8 18 2 27 145 120 148 97 146 11 219 122 20 99 124 4 71 4 35 34 1 7 102 29 1 8 112 37 3 94 4 71 10 106 130 4 73 4 27 55 28 76 4 4 32 31 1 12 33 1 21 2 54 27 158 91 144 183 3 55 4 70 4 3 94 4 70 4 29 1 9 66 2 11 30 1 12 60 3 118 113 144 200 99 124 4 78 4 40 16 17 13 1 62 1 8 104 26 20 16 17 1 7 27 158 91 7 151 36 1 8 66 2 7 33 1 10 81 39 16 17 1 7 4 32 31 1 9 101 1 8 18 15 7 33 1 8 63 107 132 4 71 6 11 30 1 9 22 27 55 121 148 97 4 32 31 1 12 10 85 1 64 16 35 34 1 9 65 7 33 1 9 19 35 34 1 18 2 27 55 120 97 155 4 32 31 1 12 4 32 31 1 8 29 1 12 92 2 4 32 31 1 12 61 16 17 1 10 4 32 31 1 9 263 179 209 141 4 7 33 1 8 58 36 1 8 137 2 106 128 91 4 28 40 16 10 23 14 62 1 8 67 2 7 33 1 8 92 4 32 31 1 9 29 1 8 18 15 7 36 1 8 92 11 30 1 12 18 11 30 1 19 2 106 130 4 73 4 235 1 115 290 180 35 34 1 10 81 36 1 9 10 81 61 16 1 13 14 62 1 9 7 102 101 1 9 195 37 40 13 14 1 10 29 1 12 67 15 35 34 1 8 67 20 16 1 13 14 3 55 4 70 4 7 33 1 9 104 27 55 121 151 97 7 33 1 12 68 62 1 8 60 2 7 33 1 9 104 4 32 31 1 10 10 38 1 64 16 106 130 4 73 4 129 96 13 14 1 4 32 31 1 8 106 130 4 28 4 84 1 13 39 16 27 204 123 15 35 27 118 149 121 97 7 62 1 10 81 20 16 1 13 14 7 201 1 20 109 29 1 21 15 25 35 34 1 60 15 11 30 1 63 15 3 55 91 4 73 40 16 10 23 14 29 1 8 18 15 10 85 1 20 16 4 32 31 1 8 11 30 1 9 60 61 13 14 16 1 106 128 91 4 78 7 90 1 20 16 36 1 9 138 2 64 16 17 1 13 7 86 1 20 16 4 32 31 1 58 29 1 8 21 15 35 34 1 12 63 29 1 9 67 15 101 1 8 67 2 20 16 17 13 1 61 16 17 1 7 35 34 1 8 63 35 34 1 9 58 3 113 121 168 97 11 30 1 8 59 3 100 4 75 4 7 87 1 64 16 7 80 1 40 16 36 1 7 102 15 29 1 12 18 2 29 1 12 63 37 29 1 60 15 111 62 1 8 19 15 116 160 171 1 196 4 32 31 1 8 36 1 8 67 2 27 55 4 71 4 36 1 9 22 102 36 1 12 67 2 3 55 146 11 193 3 55 91 28 76 7 90 1 61 16 29 1 8 19 2 7 33 1 8 18 4 32 31 1 12 170 141 131 108 258 107 126 4 28 4 107 132 4 28 4 39 16 10 23 14 36 1 60 15 25 3 113 121 151 97 4 32 31 1 10 29 1 9 104 37 29 1 58 69 15 4 32 31 1 9 62 1 8 92 2 101 1 19 37 11 20 109 10 23 14 7 86 1 64 16 11 30 1 9 67 7 33 1 8 60 7 87 1 10 16 39 16 17 1 13 3 55 10 38 4 4 32 31 1 9 35 34 1 10 81 99 124 4 71 4 7 89 1 20 16 62 1 57 2 48 7 33 1 9 65 62 105 103 9 19 99 124 4 70 4 40 109 10 23 14 7 36 1 12 59 27 55 4 70 10 35 34 1 12 18 4 32 31 1 12 106 130 4 73 6 62 1 12 19 2 3 55 4 73 4 3 100 4 75 28 101 1 60 15 25 135 4 74 4 78 33 1 59 15 25 29 1 65 37 11 33 105 103 9 19 29 1 9 65 37 107 208 120 254 2 7 86 1 20 16 152 96 10 23 14 152 96 10 23 14 39 16 1 13 14 11 30 1 8 18 10 38 1 20 13 40 16 10 23 14 107 132 4 78 4 7 36 1 8 57 27 118 113 121 168 27 118 113 121 168 62 1 12 57 2 11 30 1 69 65 29 1 8 19 15 3 55 91 4 75 7 33 1 8 58 11 30 1 249 15 35 34 1 12 19 7 88 1 61 16 29 1 66 2 49 99 127 4 73 4 33 1 58 37 11 27 158 91 7 151 7 90 1 20 16 205 229 99 10 277 20 16 17 1 7 7 33 1 12 63 62 1 9 19 2 10 85 1 39 16 7 33 1 12 67 7 80 1 39 16 10 85 1 61 16 7 90 1 13 39 29 1 19 2 49 20 16 17 13 1 40 16 10 23 14 29 1 12 19 2 84 1 13 39 16 27 55 4 28 6 29 1 8 66 15 3 143 120 97 134 36 1 8 63 37 116 100 91 4 71 62 1 12 18 2 7 33 1 104 15 62 1 8 59 2 7 33 1 8 104 35 34 1 8 18 7 33 1 12 92 106 130 4 74 6 4 32 31 1 8 4 32 31 1 112 27 94 4 73 4 29 1 8 18 2 99 240 123 2 36 39 16 17 1 84 7 62 1 8 10 29 1 12 68 15 101 1 58 69 26 20 109 10 23 156 27 94 115 110 182 7 62 1 9 58 35 34 1 9 10 7 87 1 61 16 35 34 1 10 81 62 1 63 15 25 135 4 73 4 159 11 30 1 10 81 7 87 1 7 13 116 100 91 4 75 27 55 4 77 4 106 128 91 4 77 7 33 1 8 18 35 34 1 8 66 7 86 1 61 16 20 16 17 1 13 4 32 31 1 8 62 1 8 21 2 3 158 91 144 110 3 178 22 136 29 10 82 1 40 16 40 16 17 1 13 62 1 9 104 26 29 1 12 63 2 152 96 10 23 14 39 109 17 1 13 7 87 1 10 16 4 32 31 1 8 11 30 1 12 19 62 1 66 2 50 4 32 31 1 67 27 55 10 184 110 7 33 1 12 67 7 87 1 40 13 20 16 1 13 14 3 94 4 28 4 27 1 234 2 48 7 33 1 8 59 35 34 1 10 81 11 30 1 12 60 11 30 1 9 26 61 16 17 1 7 152 96 10 23 14 101 1 12 59 2 29 1 66 15 42 101 1 9 60 2 7 33 1 12 59 29 1 21 26 98 7 33 1 8 18 29 1 9 22 102 64 16 17 1 7 27 55 4 77 4 3 55 91 6 28 35 34 1 9 60 29 1 12 65 2 4 32 31 1 65 7 33 1 12 19 3 158 91 121 97 4 32 31 1 9 10 85 1 39 16 20 109 10 23 14 4 32 31 1 9 3 94 4 28 28 64 16 17 1 7 10 82 1 20 16 7 62 1 9 10 106 130 4 74 4 29 1 8 19 2 29 1 8 21 15 4 32 31 1 18 135 4 74 4 71 35 34 1 8 66 20 16 10 23 14 7 33 1 8 21 33 1 59 2 51 29 1 10 81 69 35 34 1 18 26 33 1 63 2 48 10 85 1 61 16 99 127 38 28 76 29 1 67 37 11 29 1 8 63 37 7 36 1 58 69 152 96 10 23 14 27 55 4 73 4 29 1 9 59 26 40 16 17 1 7 29 1 9 65 2 64 16 17 1 13 4 32 31 1 19 3 160 171 1 198 11 30 1 10 81 7 33 1 9 68 235 294 116 26 29 7 33 1 12 68 27 160 146 303 197 99 124 4 77 4 29 1 8 92 2 62 1 65 2 51 11 30 1 9 104 101 1 9 59 37 7 36 1 10 81 61 13 14 16 1 11 30 1 9 60 11 30 1 8 137 36 1 9 58 37 10 38 1 64 16 33 1 67 15 42 10 38 1 20 16 27 55 10 38 4 27 145 120 151 97 3 55 4 70 4 11 30 1 8 18 129 96 10 23 14 61 16 17 1 13 36 1 21 15 25 29 1 8 92 2 20 16 17 13 1 27 55 4 70 4 35 34 1 9 117 36 1 21 15 25 7 33 1 9 58 40 16 17 1 10 4 32 31 1 9 152 96 10 23 14 27 94 4 70 6 35 34 1 12 67 7 86 1 20 13 11 30 1 8 57 4 257 131 170 141 7 90 1 39 13 10 85 1 7 13 4 32 31 1 58 7 33 1 12 18 116 160 146 7 194 99 127 4 71 28 115 217 122 40 109 27 94 10 110 133 36 1 9 10 81 20 16 17 1 7 10 82 1 39 16 29 1 8 18 2 29 1 12 65 2 20 16 17 1 7 61 16 17 1 7 36 1 8 67 2 7 33 1 60 2 29 1 9 10 81 35 34 1 9 63 27 55 120 147 97 3 100 4 72 4 4 32 31 1 249 11 30 1 249 15 7 80 1 20 16 36 1 9 22 225 29 1 63 37 11 20 16 17 1 13 11 30 1 69 65 29 1 8 57 2 27 94 4 28 28 3 55 4 28 4 11 30 1 9 67 4 32 31 1 12 62 1 8 63 2 27 55 91 28 76 35 34 1 57 2 4 32 31 105 131 99 124 4 28 4 3 55 91 120 168 62 1 8 68 2 27 94 4 28 38 27 94 4 78 4 29 1 9 21 15 35 34 1 104 2 7 87 1 40 16 39 13 14 108 96 27 55 7 168 97 20 16 1 13 14 101 1 9 60 2 35 34 1 12 57 4 32 31 1 12 7 89 1 64 16 106 130 10 38 6 27 180 173 32 31 205 131 99 91 123 35 34 1 8 65 7 36 1 8 19 101 1 60 15 25 29 1 8 65 37 33 1 66 37 11 29 1 8 18 2 20 16 17 13 1 255 122 40 254 26 101 1 8 67 15 29 1 12 19 15 11 30 1 9 66 7 33 1 8 63 27 143 120 148 97 36 1 9 66 15 20 16 17 13 1 27 94 4 71 4 116 100 91 4 77 7 33 1 8 92 3 94 4 71 4 35 34 1 12 18 35 34 1 19 15 4 32 31 1 58 3 143 115 184 110 106 130 4 70 4 11 30 1 9 117 62 1 63 37 11 3 94 4 28 4 106 128 91 4 72 4 32 31 1 12 84 1 64 16 17 7 80 1 20 16 11 30 1 9 7 7 86 1 64 16 27 55 4 71 4 3 55 91 10 200 4 32 31 1 12 36 1 9 60 2 10 82 1 40 16 10 82 1 39 13 29 1 12 18 15 106 130 10 38 4 11 30 1 58 2 99 124 4 73 10 7 83 1 7 13 35 34 1 9 63 29 1 8 18 2 11 30 1 12 57 20 16 10 23 14 35 34 1 8 18 7 80 1 39 13 7 33 1 9 104 11 30 1 8 18 3 143 121 147 97 3 100 4 74 4 11 30 1 12 19 7 86 1 61 16 61 16 17 1 7 36 1 8 18 2 7 33 1 8 137 3 55 4 74 4 4 32 31 1 8 3 145 120 97 134 7 36 1 58 12 35 34 1 8 19 27 94 4 28 4 7 89 1 20 16 39 16 10 23 14 38 131 191 96 4 27 247 121 147 97 4 32 31 1 8 27 94 4 75 4 62 1 21 2 49 36 1 8 66 2 35 34 1 12 92 39 16 17 1 7 11 30 1 9 104 27 55 4 74 4 4 32 31 1 60 36 1 9 22 102 27 55 4 77 6 3 55 4 78 4 3 55 4 77 4 7 36 1 8 10 11 30 1 67 2 35 34 1 9 58 129 96 10 23 14 4 32 31 1 58 20 16 17 13 1 106 128 91 4 78 84 1 40 16 13 3 100 4 73 4 3 1 198 22 164 4 32 31 1 9\n\t [[Node: loss_1/time_distributed_2_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss_1/time_distributed_2_loss/Reshape_1, loss_1/time_distributed_2_loss/Cast)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    517\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Received a label value of 344 which is outside the valid range of [0, 344).  Label values: 7 88 1 64 16 11 30 1 67 15 62 1 92 2 41 7 33 1 9 7 29 1 65 37 11 10 38 1 20 16 36 1 12 67 37 27 145 7 97 134 3 113 115 184 110 3 94 4 28 4 11 30 1 19 15 3 55 4 72 4 3 55 4 78 4 3 55 4 28 4 11 30 1 12 57 4 32 31 1 8 10 82 1 61 16 10 85 1 20 16 10 38 1 40 16 62 1 12 63 2 99 124 4 70 4 7 89 1 39 16 10 85 1 39 13 36 1 8 18 2 20 13 16 17 1 20 16 10 23 14 35 34 1 8 19 4 32 31 1 9 36 1 8 57 2 3 94 4 71 4 64 16 17 1 10 29 1 19 2 52 29 1 8 66 2 29 1 12 21 37 29 1 8 66 15 4 32 31 1 21 7 36 1 8 65 33 1 57 2 49 7 33 1 8 63 36 1 69 21 2 7 33 1 12 65 20 16 17 1 13 39 109 17 1 13 7 88 1 7 13 64 16 17 13 1 62 1 21 2 50 35 34 1 66 2 4 32 31 105 131 27 94 7 148 97 29 1 58 69 2 99 127 91 28 76 4 32 31 1 12 27 55 4 75 4 4 32 31 1 8 62 1 12 65 37 11 30 1 12 18 101 1 12 18 15 29 1 19 2 41 7 33 1 9 18 62 105 103 9 19 135 28 76 4 28 3 55 4 73 4 7 90 1 61 16 3 180 35 34 24 27 94 4 78 4 36 105 103 9 19 11 30 1 19 2 3 55 10 163 110 11 30 1 19 15 35 34 1 58 8 7 86 1 39 16 84 1 39 16 13 4 32 31 1 8 36 1 12 18 2 11 30 1 19 15 29 1 12 57 2 7 86 1 61 16 35 34 1 12 18 107 126 4 77 28 29 1 65 2 52 11 30 1 12 67 20 16 1 13 14 7 62 1 9 58 11 30 1 58 2 4 32 31 1 18 107 132 10 38 4 35 34 1 12 68 27 55 4 75 10 40 109 17 1 146 20 16 1 13 14 3 55 4 72 4 11 30 1 10 81 4 32 31 1 12 62 1 12 63 2 62 1 8 104 2 7 83 1 20 16 29 1 8 19 2 40 13 14 108 96 7 88 1 61 16 7 62 1 58 8 11 30 1 9 58 7 194 161 10 189 7 33 1 12 67 7 33 1 12 18 29 1 8 104 15 7 83 1 40 16 115 221 1 40 109 35 34 1 104 2 29 1 7 102 15 3 118 113 10 200 29 1 8 60 2 35 34 1 12 59 61 16 17 1 13 4 32 31 1 63 29 1 9 57 2 29 1 8 19 2 27 55 4 70 4 29 1 12 67 2 3 55 4 28 4 61 16 17 1 84 39 16 17 1 7 7 80 1 40 16 20 16 1 13 14 29 1 10 81 37 10 82 1 39 16 7 33 1 9 18 10 85 1 40 16 29 1 66 15 25 99 124 4 28 6 20 16 10 23 14 3 55 4 78 6 29 1 8 59 2 7 36 1 12 57 99 127 91 4 77 29 1 8 67 2 7 33 1 8 58 36 1 9 66 37 10 38 1 20 16 35 34 1 9 58 27 94 115 110 172 20 16 1 13 14 7 36 1 8 60 39 16 1 13 14 29 1 58 69 2 4 32 31 1 8 35 34 1 12 18 7 33 1 18 15 4 32 31 1 58 36 1 60 2 54 27 94 4 77 4 4 32 31 1 12 29 1 9 60 15 36 1 8 19 2 3 55 4 73 4 107 208 22 136 7 7 36 1 8 63 20 16 10 23 14 11 30 1 66 37 7 33 1 12 68 11 30 1 26 63 7 86 1 20 16 29 1 9 60 15 61 13 14 16 1 7 83 1 61 16 7 33 1 9 18 62 1 9 7 102 36 1 104 2 24 40 109 10 23 156 20 16 17 1 13 7 87 1 64 16 3 1 198 22 164 84 1 20 16 17 11 30 1 58 2 11 30 1 9 18 129 96 10 23 14 27 1 15 230 79 61 16 17 1 7 4 32 31 1 19 27 94 4 71 4 27 55 10 82 10 3 55 4 28 4 35 34 1 9 18 10 85 1 40 16 27 94 4 72 4 7 33 1 8 19 4 32 31 1 12 4 32 31 1 9 3 113 7 168 97 7 36 1 8 104 36 1 12 92 2 35 34 105 95 2 27 94 4 28 4 11 30 1 9 18 20 16 17 1 7 3 203 123 2 62 36 1 21 2 54 3 100 4 74 4 7 33 1 8 58 7 83 1 39 16 27 55 4 70 4 35 34 1 18 2 36 1 19 2 43 10 85 1 61 16 170 323 141 131 108 35 34 1 12 65 36 1 19 37 11 7 90 1 39 16 20 16 10 23 14 10 82 1 39 16 101 1 9 59 2 27 55 4 71 4 3 118 149 244 182 11 30 1 12 63 10 82 1 61 16 7 90 1 13 20 29 1 8 66 37 35 34 1 8 19 27 100 120 148 97 29 1 10 81 69 62 1 12 66 2 107 126 28 76 4 191 96 13 14 1 11 30 1 69 21 35 34 1 58 2 135 4 70 4 72 7 33 1 9 60 7 33 1 8 19 4 32 31 1 21 7 80 1 39 13 33 1 21 15 25 29 1 195 26 98 29 1 12 60 2 4 32 31 1 59 20 16 10 23 14 35 34 1 12 60 27 55 4 74 28 3 145 115 163 110 29 1 12 67 2 62 1 8 57 37 35 34 1 8 66 29 1 9 58 15 29 1 9 57 37 39 16 17 1 7 62 1 18 2 49 40 16 17 1 10 11 30 1 8 63 3 55 91 7 147 29 1 8 21 2 11 30 1 9 58 99 124 10 38 4 99 127 4 74 4 29 1 12 18 2 35 34 1 9 66 29 1 9 67 37 27 55 4 73 4 20 16 1 13 14 10 38 1 20 16 99 124 4 72 4 20 16 1 13 14 3 55 91 10 38 101 1 9 18 2 10 85 1 61 16 20 109 10 23 14 29 1 21 2 45 27 145 121 148 97 3 55 4 78 4 3 55 4 75 4 101 1 58 69 2 11 30 1 12 59 7 36 1 10 81 3 55 4 78 4 36 1 19 37 11 11 30 1 9 125 191 96 13 14 1 11 30 1 8 7 3 100 4 72 10 7 80 1 20 16 4 32 31 1 9 27 55 4 74 10 39 13 14 1 10 11 30 1 8 65 27 143 121 151 97 3 158 91 7 147 146 10 218 1 40 10 220 1 20 109 29 1 12 19 15 7 33 1 9 18 101 1 9 65 15 35 34 1 9 66 62 1 8 60 2 7 33 1 12 21 10 82 1 20 16 1 3 223 15 35 4 32 31 1 9 20 16 17 1 7 11 30 1 59 15 3 158 91 144 163 7 86 1 64 16 35 34 1 19 15 11 30 1 104 15 20 16 10 23 14 62 105 103 9 19 3 113 120 168 97 64 16 17 1 10 35 34 1 9 66 33 1 66 37 11 4 32 31 1 12 29 1 9 59 2 29 1 8 60 15 35 34 1 12 59 36 1 9 7 102 7 33 1 8 58 62 1 8 57 2 35 34 1 9 18 62 1 8 104 15 107 126 10 38 4 36 1 9 59 37 7 33 1 10 81 35 34 1 60 2 7 83 1 10 16 84 1 20 16 17 99 127 91 28 76 7 36 1 8 10 29 1 8 10 81 4 32 31 1 9 101 1 8 18 2 7 33 1 9 60 99 127 91 4 77 39 16 10 23 14 7 33 1 60 26 11 30 1 104 26 29 1 59 26 98 3 55 4 73 4 36 1 19 2 52 29 1 12 59 2 7 89 1 20 16 10 85 1 20 16 29 1 57 15 25 106 130 4 74 6 101 1 8 18 37 4 32 31 1 9 20 16 1 13 14 99 127 10 38 4 99 127 4 75 28 40 16 17 13 1 29 1 8 18 37 4 32 31 1 12 36 1 104 2 45 116 100 4 78 4 3 94 4 71 4 3 55 91 4 77 11 30 1 243 15 33 105 103 9 19 7 89 1 39 13 7 80 1 20 13 4 32 31 1 19 35 34 1 58 8 4 32 31 1 9 35 34 1 9 18 3 55 10 38 28 4 32 31 1 9 3 118 113 7 97 64 16 1 13 14 4 32 31 1 9 3 55 91 28 76 11 30 1 12 92 27 143 7 151 97 62 1 9 63 2 35 34 1 9 57 20 16 17 1 13 36 1 12 18 2 11 30 1 12 19 212 99 124 278 0 7 87 1 20 16 3 94 4 70 4 135 4 74 4 73 11 30 1 9 19 27 94 4 28 4 29 1 9 26 63 7 90 1 20 16 36 1 9 125 2 7 90 1 64 16 7 33 1 104 2 27 55 7 288 153 11 30 1 9 21 20 16 17 1 10 11 30 1 104 2 7 62 1 10 81 3 55 4 70 6 27 113 120 148 97 106 130 28 76 4 20 16 10 23 14 4 32 31 1 9 27 55 4 77 4 29 1 69 21 2 116 100 4 74 4 29 1 9 66 15 129 96 10 23 14 29 1 12 66 15 146 7 201 1 39 4 32 31 1 9 27 55 4 74 4 29 1 8 112 15 3 160 171 1 198 27 55 4 77 78 3 100 4 75 28 11 30 1 9 59 106 130 28 76 4 4 32 31 1 8 11 30 1 7 102 27 94 10 38 4 116 100 38 4 71 20 16 1 13 14 36 1 8 63 37 84 1 20 16 10 7 90 1 61 16 29 1 68 2 52 27 55 4 70 4 11 30 1 18 2 29 1 9 19 2 36 105 103 9 19 20 16 10 23 14 135 4 73 4 28 29 1 9 58 2 35 34 1 8 21 29 1 8 63 2 36 1 63 2 52 29 1 68 15 25 7 80 1 64 16 7 33 1 12 59 116 207 22 136 7 64 16 17 1 10 3 55 115 248 110 35 34 1 9 117 35 34 1 19 2 27 94 4 75 6 3 94 121 147 97 36 1 8 18 2 29 1 9 138 2 99 131 233 2 36 11 30 1 8 59 101 1 9 65 2 35 34 105 103 9 29 1 9 18 26 61 16 17 13 1 3 100 4 74 4 4 32 31 1 12 62 1 8 92 2 27 55 4 78 4 135 4 71 6 4 7 33 1 12 63 7 33 1 8 10 29 1 9 22 102 20 16 1 13 14 11 30 1 12 68 129 96 13 14 1 135 10 38 4 77 3 55 4 28 4 35 34 1 8 21 11 30 1 12 68 99 124 10 38 6 29 1 9 58 37 29 1 8 19 2 7 62 1 9 10 36 1 58 26 98 40 16 10 23 14 27 143 146 165 97 7 86 1 61 16 11 30 1 8 21 115 216 122 20 109 107 126 4 77 4 107 126 4 78 4 3 143 10 248 110 11 30 1 10 81 35 34 1 9 65 27 1 10 344 22 62 1 9 19 2 10 82 1 64 16 35 34 1 10 81 27 178 22 167 181 4 32 31 1 21 27 55 4 71 4 3 55 91 10 38 20 16 17 1 10 7 33 1 9 58 263 179 209 141 175 4 32 31 1 7 7 33 1 12 18 62 1 69 21 15 36 1 9 117 2 4 32 31 1 8 64 16 17 1 84 20 16 17 13 1 99 124 4 72 4 35 34 1 21 2 7 36 1 8 306 4 32 31 1 21 40 16 1 13 14 35 34 1 12 60 7 33 1 9 18 35 34 1 12 63 33 1 57 2 43 29 1 12 59 2 20 16 10 23 14 135 4 71 4 72 39 16 17 1 7 7 33 1 9 19 11 30 1 8 18 7 89 1 20 16 62 1 12 63 2 35 34 1 63 37 64 16 17 1 13 29 1 9 125 2 29 1 8 18 2 27 145 120 148 97 146 11 219 122 20 99 124 4 71 4 35 34 1 7 102 29 1 8 112 37 3 94 4 71 10 106 130 4 73 4 27 55 28 76 4 4 32 31 1 12 33 1 21 2 54 27 158 91 144 183 3 55 4 70 4 3 94 4 70 4 29 1 9 66 2 11 30 1 12 60 3 118 113 144 200 99 124 4 78 4 40 16 17 13 1 62 1 8 104 26 20 16 17 1 7 27 158 91 7 151 36 1 8 66 2 7 33 1 10 81 39 16 17 1 7 4 32 31 1 9 101 1 8 18 15 7 33 1 8 63 107 132 4 71 6 11 30 1 9 22 27 55 121 148 97 4 32 31 1 12 10 85 1 64 16 35 34 1 9 65 7 33 1 9 19 35 34 1 18 2 27 55 120 97 155 4 32 31 1 12 4 32 31 1 8 29 1 12 92 2 4 32 31 1 12 61 16 17 1 10 4 32 31 1 9 263 179 209 141 4 7 33 1 8 58 36 1 8 137 2 106 128 91 4 28 40 16 10 23 14 62 1 8 67 2 7 33 1 8 92 4 32 31 1 9 29 1 8 18 15 7 36 1 8 92 11 30 1 12 18 11 30 1 19 2 106 130 4 73 4 235 1 115 290 180 35 34 1 10 81 36 1 9 10 81 61 16 1 13 14 62 1 9 7 102 101 1 9 195 37 40 13 14 1 10 29 1 12 67 15 35 34 1 8 67 20 16 1 13 14 3 55 4 70 4 7 33 1 9 104 27 55 121 151 97 7 33 1 12 68 62 1 8 60 2 7 33 1 9 104 4 32 31 1 10 10 38 1 64 16 106 130 4 73 4 129 96 13 14 1 4 32 31 1 8 106 130 4 28 4 84 1 13 39 16 27 204 123 15 35 27 118 149 121 97 7 62 1 10 81 20 16 1 13 14 7 201 1 20 109 29 1 21 15 25 35 34 1 60 15 11 30 1 63 15 3 55 91 4 73 40 16 10 23 14 29 1 8 18 15 10 85 1 20 16 4 32 31 1 8 11 30 1 9 60 61 13 14 16 1 106 128 91 4 78 7 90 1 20 16 36 1 9 138 2 64 16 17 1 13 7 86 1 20 16 4 32 31 1 58 29 1 8 21 15 35 34 1 12 63 29 1 9 67 15 101 1 8 67 2 20 16 17 13 1 61 16 17 1 7 35 34 1 8 63 35 34 1 9 58 3 113 121 168 97 11 30 1 8 59 3 100 4 75 4 7 87 1 64 16 7 80 1 40 16 36 1 7 102 15 29 1 12 18 2 29 1 12 63 37 29 1 60 15 111 62 1 8 19 15 116 160 171 1 196 4 32 31 1 8 36 1 8 67 2 27 55 4 71 4 36 1 9 22 102 36 1 12 67 2 3 55 146 11 193 3 55 91 28 76 7 90 1 61 16 29 1 8 19 2 7 33 1 8 18 4 32 31 1 12 170 141 131 108 258 107 126 4 28 4 107 132 4 28 4 39 16 10 23 14 36 1 60 15 25 3 113 121 151 97 4 32 31 1 10 29 1 9 104 37 29 1 58 69 15 4 32 31 1 9 62 1 8 92 2 101 1 19 37 11 20 109 10 23 14 7 86 1 64 16 11 30 1 9 67 7 33 1 8 60 7 87 1 10 16 39 16 17 1 13 3 55 10 38 4 4 32 31 1 9 35 34 1 10 81 99 124 4 71 4 7 89 1 20 16 62 1 57 2 48 7 33 1 9 65 62 105 103 9 19 99 124 4 70 4 40 109 10 23 14 7 36 1 12 59 27 55 4 70 10 35 34 1 12 18 4 32 31 1 12 106 130 4 73 6 62 1 12 19 2 3 55 4 73 4 3 100 4 75 28 101 1 60 15 25 135 4 74 4 78 33 1 59 15 25 29 1 65 37 11 33 105 103 9 19 29 1 9 65 37 107 208 120 254 2 7 86 1 20 16 152 96 10 23 14 152 96 10 23 14 39 16 1 13 14 11 30 1 8 18 10 38 1 20 13 40 16 10 23 14 107 132 4 78 4 7 36 1 8 57 27 118 113 121 168 27 118 113 121 168 62 1 12 57 2 11 30 1 69 65 29 1 8 19 15 3 55 91 4 75 7 33 1 8 58 11 30 1 249 15 35 34 1 12 19 7 88 1 61 16 29 1 66 2 49 99 127 4 73 4 33 1 58 37 11 27 158 91 7 151 7 90 1 20 16 205 229 99 10 277 20 16 17 1 7 7 33 1 12 63 62 1 9 19 2 10 85 1 39 16 7 33 1 12 67 7 80 1 39 16 10 85 1 61 16 7 90 1 13 39 29 1 19 2 49 20 16 17 13 1 40 16 10 23 14 29 1 12 19 2 84 1 13 39 16 27 55 4 28 6 29 1 8 66 15 3 143 120 97 134 36 1 8 63 37 116 100 91 4 71 62 1 12 18 2 7 33 1 104 15 62 1 8 59 2 7 33 1 8 104 35 34 1 8 18 7 33 1 12 92 106 130 4 74 6 4 32 31 1 8 4 32 31 1 112 27 94 4 73 4 29 1 8 18 2 99 240 123 2 36 39 16 17 1 84 7 62 1 8 10 29 1 12 68 15 101 1 58 69 26 20 109 10 23 156 27 94 115 110 182 7 62 1 9 58 35 34 1 9 10 7 87 1 61 16 35 34 1 10 81 62 1 63 15 25 135 4 73 4 159 11 30 1 10 81 7 87 1 7 13 116 100 91 4 75 27 55 4 77 4 106 128 91 4 77 7 33 1 8 18 35 34 1 8 66 7 86 1 61 16 20 16 17 1 13 4 32 31 1 8 62 1 8 21 2 3 158 91 144 110 3 178 22 136 29 10 82 1 40 16 40 16 17 1 13 62 1 9 104 26 29 1 12 63 2 152 96 10 23 14 39 109 17 1 13 7 87 1 10 16 4 32 31 1 8 11 30 1 12 19 62 1 66 2 50 4 32 31 1 67 27 55 10 184 110 7 33 1 12 67 7 87 1 40 13 20 16 1 13 14 3 94 4 28 4 27 1 234 2 48 7 33 1 8 59 35 34 1 10 81 11 30 1 12 60 11 30 1 9 26 61 16 17 1 7 152 96 10 23 14 101 1 12 59 2 29 1 66 15 42 101 1 9 60 2 7 33 1 12 59 29 1 21 26 98 7 33 1 8 18 29 1 9 22 102 64 16 17 1 7 27 55 4 77 4 3 55 91 6 28 35 34 1 9 60 29 1 12 65 2 4 32 31 1 65 7 33 1 12 19 3 158 91 121 97 4 32 31 1 9 10 85 1 39 16 20 109 10 23 14 4 32 31 1 9 3 94 4 28 28 64 16 17 1 7 10 82 1 20 16 7 62 1 9 10 106 130 4 74 4 29 1 8 19 2 29 1 8 21 15 4 32 31 1 18 135 4 74 4 71 35 34 1 8 66 20 16 10 23 14 7 33 1 8 21 33 1 59 2 51 29 1 10 81 69 35 34 1 18 26 33 1 63 2 48 10 85 1 61 16 99 127 38 28 76 29 1 67 37 11 29 1 8 63 37 7 36 1 58 69 152 96 10 23 14 27 55 4 73 4 29 1 9 59 26 40 16 17 1 7 29 1 9 65 2 64 16 17 1 13 4 32 31 1 19 3 160 171 1 198 11 30 1 10 81 7 33 1 9 68 235 294 116 26 29 7 33 1 12 68 27 160 146 303 197 99 124 4 77 4 29 1 8 92 2 62 1 65 2 51 11 30 1 9 104 101 1 9 59 37 7 36 1 10 81 61 13 14 16 1 11 30 1 9 60 11 30 1 8 137 36 1 9 58 37 10 38 1 64 16 33 1 67 15 42 10 38 1 20 16 27 55 10 38 4 27 145 120 151 97 3 55 4 70 4 11 30 1 8 18 129 96 10 23 14 61 16 17 1 13 36 1 21 15 25 29 1 8 92 2 20 16 17 13 1 27 55 4 70 4 35 34 1 9 117 36 1 21 15 25 7 33 1 9 58 40 16 17 1 10 4 32 31 1 9 152 96 10 23 14 27 94 4 70 6 35 34 1 12 67 7 86 1 20 13 11 30 1 8 57 4 257 131 170 141 7 90 1 39 13 10 85 1 7 13 4 32 31 1 58 7 33 1 12 18 116 160 146 7 194 99 127 4 71 28 115 217 122 40 109 27 94 10 110 133 36 1 9 10 81 20 16 17 1 7 10 82 1 39 16 29 1 8 18 2 29 1 12 65 2 20 16 17 1 7 61 16 17 1 7 36 1 8 67 2 7 33 1 60 2 29 1 9 10 81 35 34 1 9 63 27 55 120 147 97 3 100 4 72 4 4 32 31 1 249 11 30 1 249 15 7 80 1 20 16 36 1 9 22 225 29 1 63 37 11 20 16 17 1 13 11 30 1 69 65 29 1 8 57 2 27 94 4 28 28 3 55 4 28 4 11 30 1 9 67 4 32 31 1 12 62 1 8 63 2 27 55 91 28 76 35 34 1 57 2 4 32 31 105 131 99 124 4 28 4 3 55 91 120 168 62 1 8 68 2 27 94 4 28 38 27 94 4 78 4 29 1 9 21 15 35 34 1 104 2 7 87 1 40 16 39 13 14 108 96 27 55 7 168 97 20 16 1 13 14 101 1 9 60 2 35 34 1 12 57 4 32 31 1 12 7 89 1 64 16 106 130 10 38 6 27 180 173 32 31 205 131 99 91 123 35 34 1 8 65 7 36 1 8 19 101 1 60 15 25 29 1 8 65 37 33 1 66 37 11 29 1 8 18 2 20 16 17 13 1 255 122 40 254 26 101 1 8 67 15 29 1 12 19 15 11 30 1 9 66 7 33 1 8 63 27 143 120 148 97 36 1 9 66 15 20 16 17 13 1 27 94 4 71 4 116 100 91 4 77 7 33 1 8 92 3 94 4 71 4 35 34 1 12 18 35 34 1 19 15 4 32 31 1 58 3 143 115 184 110 106 130 4 70 4 11 30 1 9 117 62 1 63 37 11 3 94 4 28 4 106 128 91 4 72 4 32 31 1 12 84 1 64 16 17 7 80 1 20 16 11 30 1 9 7 7 86 1 64 16 27 55 4 71 4 3 55 91 10 200 4 32 31 1 12 36 1 9 60 2 10 82 1 40 16 10 82 1 39 13 29 1 12 18 15 106 130 10 38 4 11 30 1 58 2 99 124 4 73 10 7 83 1 7 13 35 34 1 9 63 29 1 8 18 2 11 30 1 12 57 20 16 10 23 14 35 34 1 8 18 7 80 1 39 13 7 33 1 9 104 11 30 1 8 18 3 143 121 147 97 3 100 4 74 4 11 30 1 12 19 7 86 1 61 16 61 16 17 1 7 36 1 8 18 2 7 33 1 8 137 3 55 4 74 4 4 32 31 1 8 3 145 120 97 134 7 36 1 58 12 35 34 1 8 19 27 94 4 28 4 7 89 1 20 16 39 16 10 23 14 38 131 191 96 4 27 247 121 147 97 4 32 31 1 8 27 94 4 75 4 62 1 21 2 49 36 1 8 66 2 35 34 1 12 92 39 16 17 1 7 11 30 1 9 104 27 55 4 74 4 4 32 31 1 60 36 1 9 22 102 27 55 4 77 6 3 55 4 78 4 3 55 4 77 4 7 36 1 8 10 11 30 1 67 2 35 34 1 9 58 129 96 10 23 14 4 32 31 1 58 20 16 17 13 1 106 128 91 4 78 84 1 40 16 13 3 100 4 73 4 3 1 198 22 164 4 32 31 1 9\n\t [[Node: loss_1/time_distributed_2_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss_1/time_distributed_2_loss/Reshape_1, loss_1/time_distributed_2_loss/Cast)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-08006f21ec40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0menglish_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     french_vocab_size)\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0msimple_rnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreproc_french_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# Print prediction(s)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1247\u001b[0m                             val_outs = self._test_loop(val_f, val_ins,\n\u001b[1;32m   1248\u001b[0m                                                        \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m                                                        verbose=0)\n\u001b[0m\u001b[1;32m   1250\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m                                 \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_test_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1424\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1427\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Received a label value of 344 which is outside the valid range of [0, 344).  Label values: 7 88 1 64 16 11 30 1 67 15 62 1 92 2 41 7 33 1 9 7 29 1 65 37 11 10 38 1 20 16 36 1 12 67 37 27 145 7 97 134 3 113 115 184 110 3 94 4 28 4 11 30 1 19 15 3 55 4 72 4 3 55 4 78 4 3 55 4 28 4 11 30 1 12 57 4 32 31 1 8 10 82 1 61 16 10 85 1 20 16 10 38 1 40 16 62 1 12 63 2 99 124 4 70 4 7 89 1 39 16 10 85 1 39 13 36 1 8 18 2 20 13 16 17 1 20 16 10 23 14 35 34 1 8 19 4 32 31 1 9 36 1 8 57 2 3 94 4 71 4 64 16 17 1 10 29 1 19 2 52 29 1 8 66 2 29 1 12 21 37 29 1 8 66 15 4 32 31 1 21 7 36 1 8 65 33 1 57 2 49 7 33 1 8 63 36 1 69 21 2 7 33 1 12 65 20 16 17 1 13 39 109 17 1 13 7 88 1 7 13 64 16 17 13 1 62 1 21 2 50 35 34 1 66 2 4 32 31 105 131 27 94 7 148 97 29 1 58 69 2 99 127 91 28 76 4 32 31 1 12 27 55 4 75 4 4 32 31 1 8 62 1 12 65 37 11 30 1 12 18 101 1 12 18 15 29 1 19 2 41 7 33 1 9 18 62 105 103 9 19 135 28 76 4 28 3 55 4 73 4 7 90 1 61 16 3 180 35 34 24 27 94 4 78 4 36 105 103 9 19 11 30 1 19 2 3 55 10 163 110 11 30 1 19 15 35 34 1 58 8 7 86 1 39 16 84 1 39 16 13 4 32 31 1 8 36 1 12 18 2 11 30 1 19 15 29 1 12 57 2 7 86 1 61 16 35 34 1 12 18 107 126 4 77 28 29 1 65 2 52 11 30 1 12 67 20 16 1 13 14 7 62 1 9 58 11 30 1 58 2 4 32 31 1 18 107 132 10 38 4 35 34 1 12 68 27 55 4 75 10 40 109 17 1 146 20 16 1 13 14 3 55 4 72 4 11 30 1 10 81 4 32 31 1 12 62 1 12 63 2 62 1 8 104 2 7 83 1 20 16 29 1 8 19 2 40 13 14 108 96 7 88 1 61 16 7 62 1 58 8 11 30 1 9 58 7 194 161 10 189 7 33 1 12 67 7 33 1 12 18 29 1 8 104 15 7 83 1 40 16 115 221 1 40 109 35 34 1 104 2 29 1 7 102 15 3 118 113 10 200 29 1 8 60 2 35 34 1 12 59 61 16 17 1 13 4 32 31 1 63 29 1 9 57 2 29 1 8 19 2 27 55 4 70 4 29 1 12 67 2 3 55 4 28 4 61 16 17 1 84 39 16 17 1 7 7 80 1 40 16 20 16 1 13 14 29 1 10 81 37 10 82 1 39 16 7 33 1 9 18 10 85 1 40 16 29 1 66 15 25 99 124 4 28 6 20 16 10 23 14 3 55 4 78 6 29 1 8 59 2 7 36 1 12 57 99 127 91 4 77 29 1 8 67 2 7 33 1 8 58 36 1 9 66 37 10 38 1 20 16 35 34 1 9 58 27 94 115 110 172 20 16 1 13 14 7 36 1 8 60 39 16 1 13 14 29 1 58 69 2 4 32 31 1 8 35 34 1 12 18 7 33 1 18 15 4 32 31 1 58 36 1 60 2 54 27 94 4 77 4 4 32 31 1 12 29 1 9 60 15 36 1 8 19 2 3 55 4 73 4 107 208 22 136 7 7 36 1 8 63 20 16 10 23 14 11 30 1 66 37 7 33 1 12 68 11 30 1 26 63 7 86 1 20 16 29 1 9 60 15 61 13 14 16 1 7 83 1 61 16 7 33 1 9 18 62 1 9 7 102 36 1 104 2 24 40 109 10 23 156 20 16 17 1 13 7 87 1 64 16 3 1 198 22 164 84 1 20 16 17 11 30 1 58 2 11 30 1 9 18 129 96 10 23 14 27 1 15 230 79 61 16 17 1 7 4 32 31 1 19 27 94 4 71 4 27 55 10 82 10 3 55 4 28 4 35 34 1 9 18 10 85 1 40 16 27 94 4 72 4 7 33 1 8 19 4 32 31 1 12 4 32 31 1 9 3 113 7 168 97 7 36 1 8 104 36 1 12 92 2 35 34 105 95 2 27 94 4 28 4 11 30 1 9 18 20 16 17 1 7 3 203 123 2 62 36 1 21 2 54 3 100 4 74 4 7 33 1 8 58 7 83 1 39 16 27 55 4 70 4 35 34 1 18 2 36 1 19 2 43 10 85 1 61 16 170 323 141 131 108 35 34 1 12 65 36 1 19 37 11 7 90 1 39 16 20 16 10 23 14 10 82 1 39 16 101 1 9 59 2 27 55 4 71 4 3 118 149 244 182 11 30 1 12 63 10 82 1 61 16 7 90 1 13 20 29 1 8 66 37 35 34 1 8 19 27 100 120 148 97 29 1 10 81 69 62 1 12 66 2 107 126 28 76 4 191 96 13 14 1 11 30 1 69 21 35 34 1 58 2 135 4 70 4 72 7 33 1 9 60 7 33 1 8 19 4 32 31 1 21 7 80 1 39 13 33 1 21 15 25 29 1 195 26 98 29 1 12 60 2 4 32 31 1 59 20 16 10 23 14 35 34 1 12 60 27 55 4 74 28 3 145 115 163 110 29 1 12 67 2 62 1 8 57 37 35 34 1 8 66 29 1 9 58 15 29 1 9 57 37 39 16 17 1 7 62 1 18 2 49 40 16 17 1 10 11 30 1 8 63 3 55 91 7 147 29 1 8 21 2 11 30 1 9 58 99 124 10 38 4 99 127 4 74 4 29 1 12 18 2 35 34 1 9 66 29 1 9 67 37 27 55 4 73 4 20 16 1 13 14 10 38 1 20 16 99 124 4 72 4 20 16 1 13 14 3 55 91 10 38 101 1 9 18 2 10 85 1 61 16 20 109 10 23 14 29 1 21 2 45 27 145 121 148 97 3 55 4 78 4 3 55 4 75 4 101 1 58 69 2 11 30 1 12 59 7 36 1 10 81 3 55 4 78 4 36 1 19 37 11 11 30 1 9 125 191 96 13 14 1 11 30 1 8 7 3 100 4 72 10 7 80 1 20 16 4 32 31 1 9 27 55 4 74 10 39 13 14 1 10 11 30 1 8 65 27 143 121 151 97 3 158 91 7 147 146 10 218 1 40 10 220 1 20 109 29 1 12 19 15 7 33 1 9 18 101 1 9 65 15 35 34 1 9 66 62 1 8 60 2 7 33 1 12 21 10 82 1 20 16 1 3 223 15 35 4 32 31 1 9 20 16 17 1 7 11 30 1 59 15 3 158 91 144 163 7 86 1 64 16 35 34 1 19 15 11 30 1 104 15 20 16 10 23 14 62 105 103 9 19 3 113 120 168 97 64 16 17 1 10 35 34 1 9 66 33 1 66 37 11 4 32 31 1 12 29 1 9 59 2 29 1 8 60 15 35 34 1 12 59 36 1 9 7 102 7 33 1 8 58 62 1 8 57 2 35 34 1 9 18 62 1 8 104 15 107 126 10 38 4 36 1 9 59 37 7 33 1 10 81 35 34 1 60 2 7 83 1 10 16 84 1 20 16 17 99 127 91 28 76 7 36 1 8 10 29 1 8 10 81 4 32 31 1 9 101 1 8 18 2 7 33 1 9 60 99 127 91 4 77 39 16 10 23 14 7 33 1 60 26 11 30 1 104 26 29 1 59 26 98 3 55 4 73 4 36 1 19 2 52 29 1 12 59 2 7 89 1 20 16 10 85 1 20 16 29 1 57 15 25 106 130 4 74 6 101 1 8 18 37 4 32 31 1 9 20 16 1 13 14 99 127 10 38 4 99 127 4 75 28 40 16 17 13 1 29 1 8 18 37 4 32 31 1 12 36 1 104 2 45 116 100 4 78 4 3 94 4 71 4 3 55 91 4 77 11 30 1 243 15 33 105 103 9 19 7 89 1 39 13 7 80 1 20 13 4 32 31 1 19 35 34 1 58 8 4 32 31 1 9 35 34 1 9 18 3 55 10 38 28 4 32 31 1 9 3 118 113 7 97 64 16 1 13 14 4 32 31 1 9 3 55 91 28 76 11 30 1 12 92 27 143 7 151 97 62 1 9 63 2 35 34 1 9 57 20 16 17 1 13 36 1 12 18 2 11 30 1 12 19 212 99 124 278 0 7 87 1 20 16 3 94 4 70 4 135 4 74 4 73 11 30 1 9 19 27 94 4 28 4 29 1 9 26 63 7 90 1 20 16 36 1 9 125 2 7 90 1 64 16 7 33 1 104 2 27 55 7 288 153 11 30 1 9 21 20 16 17 1 10 11 30 1 104 2 7 62 1 10 81 3 55 4 70 6 27 113 120 148 97 106 130 28 76 4 20 16 10 23 14 4 32 31 1 9 27 55 4 77 4 29 1 69 21 2 116 100 4 74 4 29 1 9 66 15 129 96 10 23 14 29 1 12 66 15 146 7 201 1 39 4 32 31 1 9 27 55 4 74 4 29 1 8 112 15 3 160 171 1 198 27 55 4 77 78 3 100 4 75 28 11 30 1 9 59 106 130 28 76 4 4 32 31 1 8 11 30 1 7 102 27 94 10 38 4 116 100 38 4 71 20 16 1 13 14 36 1 8 63 37 84 1 20 16 10 7 90 1 61 16 29 1 68 2 52 27 55 4 70 4 11 30 1 18 2 29 1 9 19 2 36 105 103 9 19 20 16 10 23 14 135 4 73 4 28 29 1 9 58 2 35 34 1 8 21 29 1 8 63 2 36 1 63 2 52 29 1 68 15 25 7 80 1 64 16 7 33 1 12 59 116 207 22 136 7 64 16 17 1 10 3 55 115 248 110 35 34 1 9 117 35 34 1 19 2 27 94 4 75 6 3 94 121 147 97 36 1 8 18 2 29 1 9 138 2 99 131 233 2 36 11 30 1 8 59 101 1 9 65 2 35 34 105 103 9 29 1 9 18 26 61 16 17 13 1 3 100 4 74 4 4 32 31 1 12 62 1 8 92 2 27 55 4 78 4 135 4 71 6 4 7 33 1 12 63 7 33 1 8 10 29 1 9 22 102 20 16 1 13 14 11 30 1 12 68 129 96 13 14 1 135 10 38 4 77 3 55 4 28 4 35 34 1 8 21 11 30 1 12 68 99 124 10 38 6 29 1 9 58 37 29 1 8 19 2 7 62 1 9 10 36 1 58 26 98 40 16 10 23 14 27 143 146 165 97 7 86 1 61 16 11 30 1 8 21 115 216 122 20 109 107 126 4 77 4 107 126 4 78 4 3 143 10 248 110 11 30 1 10 81 35 34 1 9 65 27 1 10 344 22 62 1 9 19 2 10 82 1 64 16 35 34 1 10 81 27 178 22 167 181 4 32 31 1 21 27 55 4 71 4 3 55 91 10 38 20 16 17 1 10 7 33 1 9 58 263 179 209 141 175 4 32 31 1 7 7 33 1 12 18 62 1 69 21 15 36 1 9 117 2 4 32 31 1 8 64 16 17 1 84 20 16 17 13 1 99 124 4 72 4 35 34 1 21 2 7 36 1 8 306 4 32 31 1 21 40 16 1 13 14 35 34 1 12 60 7 33 1 9 18 35 34 1 12 63 33 1 57 2 43 29 1 12 59 2 20 16 10 23 14 135 4 71 4 72 39 16 17 1 7 7 33 1 9 19 11 30 1 8 18 7 89 1 20 16 62 1 12 63 2 35 34 1 63 37 64 16 17 1 13 29 1 9 125 2 29 1 8 18 2 27 145 120 148 97 146 11 219 122 20 99 124 4 71 4 35 34 1 7 102 29 1 8 112 37 3 94 4 71 10 106 130 4 73 4 27 55 28 76 4 4 32 31 1 12 33 1 21 2 54 27 158 91 144 183 3 55 4 70 4 3 94 4 70 4 29 1 9 66 2 11 30 1 12 60 3 118 113 144 200 99 124 4 78 4 40 16 17 13 1 62 1 8 104 26 20 16 17 1 7 27 158 91 7 151 36 1 8 66 2 7 33 1 10 81 39 16 17 1 7 4 32 31 1 9 101 1 8 18 15 7 33 1 8 63 107 132 4 71 6 11 30 1 9 22 27 55 121 148 97 4 32 31 1 12 10 85 1 64 16 35 34 1 9 65 7 33 1 9 19 35 34 1 18 2 27 55 120 97 155 4 32 31 1 12 4 32 31 1 8 29 1 12 92 2 4 32 31 1 12 61 16 17 1 10 4 32 31 1 9 263 179 209 141 4 7 33 1 8 58 36 1 8 137 2 106 128 91 4 28 40 16 10 23 14 62 1 8 67 2 7 33 1 8 92 4 32 31 1 9 29 1 8 18 15 7 36 1 8 92 11 30 1 12 18 11 30 1 19 2 106 130 4 73 4 235 1 115 290 180 35 34 1 10 81 36 1 9 10 81 61 16 1 13 14 62 1 9 7 102 101 1 9 195 37 40 13 14 1 10 29 1 12 67 15 35 34 1 8 67 20 16 1 13 14 3 55 4 70 4 7 33 1 9 104 27 55 121 151 97 7 33 1 12 68 62 1 8 60 2 7 33 1 9 104 4 32 31 1 10 10 38 1 64 16 106 130 4 73 4 129 96 13 14 1 4 32 31 1 8 106 130 4 28 4 84 1 13 39 16 27 204 123 15 35 27 118 149 121 97 7 62 1 10 81 20 16 1 13 14 7 201 1 20 109 29 1 21 15 25 35 34 1 60 15 11 30 1 63 15 3 55 91 4 73 40 16 10 23 14 29 1 8 18 15 10 85 1 20 16 4 32 31 1 8 11 30 1 9 60 61 13 14 16 1 106 128 91 4 78 7 90 1 20 16 36 1 9 138 2 64 16 17 1 13 7 86 1 20 16 4 32 31 1 58 29 1 8 21 15 35 34 1 12 63 29 1 9 67 15 101 1 8 67 2 20 16 17 13 1 61 16 17 1 7 35 34 1 8 63 35 34 1 9 58 3 113 121 168 97 11 30 1 8 59 3 100 4 75 4 7 87 1 64 16 7 80 1 40 16 36 1 7 102 15 29 1 12 18 2 29 1 12 63 37 29 1 60 15 111 62 1 8 19 15 116 160 171 1 196 4 32 31 1 8 36 1 8 67 2 27 55 4 71 4 36 1 9 22 102 36 1 12 67 2 3 55 146 11 193 3 55 91 28 76 7 90 1 61 16 29 1 8 19 2 7 33 1 8 18 4 32 31 1 12 170 141 131 108 258 107 126 4 28 4 107 132 4 28 4 39 16 10 23 14 36 1 60 15 25 3 113 121 151 97 4 32 31 1 10 29 1 9 104 37 29 1 58 69 15 4 32 31 1 9 62 1 8 92 2 101 1 19 37 11 20 109 10 23 14 7 86 1 64 16 11 30 1 9 67 7 33 1 8 60 7 87 1 10 16 39 16 17 1 13 3 55 10 38 4 4 32 31 1 9 35 34 1 10 81 99 124 4 71 4 7 89 1 20 16 62 1 57 2 48 7 33 1 9 65 62 105 103 9 19 99 124 4 70 4 40 109 10 23 14 7 36 1 12 59 27 55 4 70 10 35 34 1 12 18 4 32 31 1 12 106 130 4 73 6 62 1 12 19 2 3 55 4 73 4 3 100 4 75 28 101 1 60 15 25 135 4 74 4 78 33 1 59 15 25 29 1 65 37 11 33 105 103 9 19 29 1 9 65 37 107 208 120 254 2 7 86 1 20 16 152 96 10 23 14 152 96 10 23 14 39 16 1 13 14 11 30 1 8 18 10 38 1 20 13 40 16 10 23 14 107 132 4 78 4 7 36 1 8 57 27 118 113 121 168 27 118 113 121 168 62 1 12 57 2 11 30 1 69 65 29 1 8 19 15 3 55 91 4 75 7 33 1 8 58 11 30 1 249 15 35 34 1 12 19 7 88 1 61 16 29 1 66 2 49 99 127 4 73 4 33 1 58 37 11 27 158 91 7 151 7 90 1 20 16 205 229 99 10 277 20 16 17 1 7 7 33 1 12 63 62 1 9 19 2 10 85 1 39 16 7 33 1 12 67 7 80 1 39 16 10 85 1 61 16 7 90 1 13 39 29 1 19 2 49 20 16 17 13 1 40 16 10 23 14 29 1 12 19 2 84 1 13 39 16 27 55 4 28 6 29 1 8 66 15 3 143 120 97 134 36 1 8 63 37 116 100 91 4 71 62 1 12 18 2 7 33 1 104 15 62 1 8 59 2 7 33 1 8 104 35 34 1 8 18 7 33 1 12 92 106 130 4 74 6 4 32 31 1 8 4 32 31 1 112 27 94 4 73 4 29 1 8 18 2 99 240 123 2 36 39 16 17 1 84 7 62 1 8 10 29 1 12 68 15 101 1 58 69 26 20 109 10 23 156 27 94 115 110 182 7 62 1 9 58 35 34 1 9 10 7 87 1 61 16 35 34 1 10 81 62 1 63 15 25 135 4 73 4 159 11 30 1 10 81 7 87 1 7 13 116 100 91 4 75 27 55 4 77 4 106 128 91 4 77 7 33 1 8 18 35 34 1 8 66 7 86 1 61 16 20 16 17 1 13 4 32 31 1 8 62 1 8 21 2 3 158 91 144 110 3 178 22 136 29 10 82 1 40 16 40 16 17 1 13 62 1 9 104 26 29 1 12 63 2 152 96 10 23 14 39 109 17 1 13 7 87 1 10 16 4 32 31 1 8 11 30 1 12 19 62 1 66 2 50 4 32 31 1 67 27 55 10 184 110 7 33 1 12 67 7 87 1 40 13 20 16 1 13 14 3 94 4 28 4 27 1 234 2 48 7 33 1 8 59 35 34 1 10 81 11 30 1 12 60 11 30 1 9 26 61 16 17 1 7 152 96 10 23 14 101 1 12 59 2 29 1 66 15 42 101 1 9 60 2 7 33 1 12 59 29 1 21 26 98 7 33 1 8 18 29 1 9 22 102 64 16 17 1 7 27 55 4 77 4 3 55 91 6 28 35 34 1 9 60 29 1 12 65 2 4 32 31 1 65 7 33 1 12 19 3 158 91 121 97 4 32 31 1 9 10 85 1 39 16 20 109 10 23 14 4 32 31 1 9 3 94 4 28 28 64 16 17 1 7 10 82 1 20 16 7 62 1 9 10 106 130 4 74 4 29 1 8 19 2 29 1 8 21 15 4 32 31 1 18 135 4 74 4 71 35 34 1 8 66 20 16 10 23 14 7 33 1 8 21 33 1 59 2 51 29 1 10 81 69 35 34 1 18 26 33 1 63 2 48 10 85 1 61 16 99 127 38 28 76 29 1 67 37 11 29 1 8 63 37 7 36 1 58 69 152 96 10 23 14 27 55 4 73 4 29 1 9 59 26 40 16 17 1 7 29 1 9 65 2 64 16 17 1 13 4 32 31 1 19 3 160 171 1 198 11 30 1 10 81 7 33 1 9 68 235 294 116 26 29 7 33 1 12 68 27 160 146 303 197 99 124 4 77 4 29 1 8 92 2 62 1 65 2 51 11 30 1 9 104 101 1 9 59 37 7 36 1 10 81 61 13 14 16 1 11 30 1 9 60 11 30 1 8 137 36 1 9 58 37 10 38 1 64 16 33 1 67 15 42 10 38 1 20 16 27 55 10 38 4 27 145 120 151 97 3 55 4 70 4 11 30 1 8 18 129 96 10 23 14 61 16 17 1 13 36 1 21 15 25 29 1 8 92 2 20 16 17 13 1 27 55 4 70 4 35 34 1 9 117 36 1 21 15 25 7 33 1 9 58 40 16 17 1 10 4 32 31 1 9 152 96 10 23 14 27 94 4 70 6 35 34 1 12 67 7 86 1 20 13 11 30 1 8 57 4 257 131 170 141 7 90 1 39 13 10 85 1 7 13 4 32 31 1 58 7 33 1 12 18 116 160 146 7 194 99 127 4 71 28 115 217 122 40 109 27 94 10 110 133 36 1 9 10 81 20 16 17 1 7 10 82 1 39 16 29 1 8 18 2 29 1 12 65 2 20 16 17 1 7 61 16 17 1 7 36 1 8 67 2 7 33 1 60 2 29 1 9 10 81 35 34 1 9 63 27 55 120 147 97 3 100 4 72 4 4 32 31 1 249 11 30 1 249 15 7 80 1 20 16 36 1 9 22 225 29 1 63 37 11 20 16 17 1 13 11 30 1 69 65 29 1 8 57 2 27 94 4 28 28 3 55 4 28 4 11 30 1 9 67 4 32 31 1 12 62 1 8 63 2 27 55 91 28 76 35 34 1 57 2 4 32 31 105 131 99 124 4 28 4 3 55 91 120 168 62 1 8 68 2 27 94 4 28 38 27 94 4 78 4 29 1 9 21 15 35 34 1 104 2 7 87 1 40 16 39 13 14 108 96 27 55 7 168 97 20 16 1 13 14 101 1 9 60 2 35 34 1 12 57 4 32 31 1 12 7 89 1 64 16 106 130 10 38 6 27 180 173 32 31 205 131 99 91 123 35 34 1 8 65 7 36 1 8 19 101 1 60 15 25 29 1 8 65 37 33 1 66 37 11 29 1 8 18 2 20 16 17 13 1 255 122 40 254 26 101 1 8 67 15 29 1 12 19 15 11 30 1 9 66 7 33 1 8 63 27 143 120 148 97 36 1 9 66 15 20 16 17 13 1 27 94 4 71 4 116 100 91 4 77 7 33 1 8 92 3 94 4 71 4 35 34 1 12 18 35 34 1 19 15 4 32 31 1 58 3 143 115 184 110 106 130 4 70 4 11 30 1 9 117 62 1 63 37 11 3 94 4 28 4 106 128 91 4 72 4 32 31 1 12 84 1 64 16 17 7 80 1 20 16 11 30 1 9 7 7 86 1 64 16 27 55 4 71 4 3 55 91 10 200 4 32 31 1 12 36 1 9 60 2 10 82 1 40 16 10 82 1 39 13 29 1 12 18 15 106 130 10 38 4 11 30 1 58 2 99 124 4 73 10 7 83 1 7 13 35 34 1 9 63 29 1 8 18 2 11 30 1 12 57 20 16 10 23 14 35 34 1 8 18 7 80 1 39 13 7 33 1 9 104 11 30 1 8 18 3 143 121 147 97 3 100 4 74 4 11 30 1 12 19 7 86 1 61 16 61 16 17 1 7 36 1 8 18 2 7 33 1 8 137 3 55 4 74 4 4 32 31 1 8 3 145 120 97 134 7 36 1 58 12 35 34 1 8 19 27 94 4 28 4 7 89 1 20 16 39 16 10 23 14 38 131 191 96 4 27 247 121 147 97 4 32 31 1 8 27 94 4 75 4 62 1 21 2 49 36 1 8 66 2 35 34 1 12 92 39 16 17 1 7 11 30 1 9 104 27 55 4 74 4 4 32 31 1 60 36 1 9 22 102 27 55 4 77 6 3 55 4 78 4 3 55 4 77 4 7 36 1 8 10 11 30 1 67 2 35 34 1 9 58 129 96 10 23 14 4 32 31 1 58 20 16 17 13 1 106 128 91 4 78 84 1 40 16 13 3 100 4 73 4 3 1 198 22 164 4 32 31 1 9\n\t [[Node: loss_1/time_distributed_2_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss_1/time_distributed_2_loss/Reshape_1, loss_1/time_distributed_2_loss/Cast)]]\n\nCaused by op 'loss_1/time_distributed_2_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits', defined at:\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/asyncio/base_events.py\", line 1425, in _run_once\n    handle._run()\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/asyncio/events.py\", line 126, in _run\n    self._callback(*self._args)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-08006f21ec40>\", line 64, in <module>\n    french_vocab_size)\n  File \"<ipython-input-13-08006f21ec40>\", line 25, in simple_model\n    model.compile(loss=sparse_categorical_crossentropy, optimizer=Adam(learning_rate), metrics=['accuracy'])\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/keras/models.py\", line 824, in compile\n    **kwargs)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/keras/engine/training.py\", line 830, in compile\n    sample_weight, mask)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/keras/engine/training.py\", line 429, in weighted\n    score_array = fn(y_true, y_pred)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/keras/losses.py\", line 73, in sparse_categorical_crossentropy\n    return K.sparse_categorical_crossentropy(y_true, y_pred)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 3041, in sparse_categorical_crossentropy\n    logits=logits)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 2052, in sparse_softmax_cross_entropy_with_logits\n    precise_logits, labels, name=name)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 7451, in sparse_softmax_cross_entropy_with_logits\n    labels=labels, name=name)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3290, in create_op\n    op_def=op_def)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1654, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Received a label value of 344 which is outside the valid range of [0, 344).  Label values: 7 88 1 64 16 11 30 1 67 15 62 1 92 2 41 7 33 1 9 7 29 1 65 37 11 10 38 1 20 16 36 1 12 67 37 27 145 7 97 134 3 113 115 184 110 3 94 4 28 4 11 30 1 19 15 3 55 4 72 4 3 55 4 78 4 3 55 4 28 4 11 30 1 12 57 4 32 31 1 8 10 82 1 61 16 10 85 1 20 16 10 38 1 40 16 62 1 12 63 2 99 124 4 70 4 7 89 1 39 16 10 85 1 39 13 36 1 8 18 2 20 13 16 17 1 20 16 10 23 14 35 34 1 8 19 4 32 31 1 9 36 1 8 57 2 3 94 4 71 4 64 16 17 1 10 29 1 19 2 52 29 1 8 66 2 29 1 12 21 37 29 1 8 66 15 4 32 31 1 21 7 36 1 8 65 33 1 57 2 49 7 33 1 8 63 36 1 69 21 2 7 33 1 12 65 20 16 17 1 13 39 109 17 1 13 7 88 1 7 13 64 16 17 13 1 62 1 21 2 50 35 34 1 66 2 4 32 31 105 131 27 94 7 148 97 29 1 58 69 2 99 127 91 28 76 4 32 31 1 12 27 55 4 75 4 4 32 31 1 8 62 1 12 65 37 11 30 1 12 18 101 1 12 18 15 29 1 19 2 41 7 33 1 9 18 62 105 103 9 19 135 28 76 4 28 3 55 4 73 4 7 90 1 61 16 3 180 35 34 24 27 94 4 78 4 36 105 103 9 19 11 30 1 19 2 3 55 10 163 110 11 30 1 19 15 35 34 1 58 8 7 86 1 39 16 84 1 39 16 13 4 32 31 1 8 36 1 12 18 2 11 30 1 19 15 29 1 12 57 2 7 86 1 61 16 35 34 1 12 18 107 126 4 77 28 29 1 65 2 52 11 30 1 12 67 20 16 1 13 14 7 62 1 9 58 11 30 1 58 2 4 32 31 1 18 107 132 10 38 4 35 34 1 12 68 27 55 4 75 10 40 109 17 1 146 20 16 1 13 14 3 55 4 72 4 11 30 1 10 81 4 32 31 1 12 62 1 12 63 2 62 1 8 104 2 7 83 1 20 16 29 1 8 19 2 40 13 14 108 96 7 88 1 61 16 7 62 1 58 8 11 30 1 9 58 7 194 161 10 189 7 33 1 12 67 7 33 1 12 18 29 1 8 104 15 7 83 1 40 16 115 221 1 40 109 35 34 1 104 2 29 1 7 102 15 3 118 113 10 200 29 1 8 60 2 35 34 1 12 59 61 16 17 1 13 4 32 31 1 63 29 1 9 57 2 29 1 8 19 2 27 55 4 70 4 29 1 12 67 2 3 55 4 28 4 61 16 17 1 84 39 16 17 1 7 7 80 1 40 16 20 16 1 13 14 29 1 10 81 37 10 82 1 39 16 7 33 1 9 18 10 85 1 40 16 29 1 66 15 25 99 124 4 28 6 20 16 10 23 14 3 55 4 78 6 29 1 8 59 2 7 36 1 12 57 99 127 91 4 77 29 1 8 67 2 7 33 1 8 58 36 1 9 66 37 10 38 1 20 16 35 34 1 9 58 27 94 115 110 172 20 16 1 13 14 7 36 1 8 60 39 16 1 13 14 29 1 58 69 2 4 32 31 1 8 35 34 1 12 18 7 33 1 18 15 4 32 31 1 58 36 1 60 2 54 27 94 4 77 4 4 32 31 1 12 29 1 9 60 15 36 1 8 19 2 3 55 4 73 4 107 208 22 136 7 7 36 1 8 63 20 16 10 23 14 11 30 1 66 37 7 33 1 12 68 11 30 1 26 63 7 86 1 20 16 29 1 9 60 15 61 13 14 16 1 7 83 1 61 16 7 33 1 9 18 62 1 9 7 102 36 1 104 2 24 40 109 10 23 156 20 16 17 1 13 7 87 1 64 16 3 1 198 22 164 84 1 20 16 17 11 30 1 58 2 11 30 1 9 18 129 96 10 23 14 27 1 15 230 79 61 16 17 1 7 4 32 31 1 19 27 94 4 71 4 27 55 10 82 10 3 55 4 28 4 35 34 1 9 18 10 85 1 40 16 27 94 4 72 4 7 33 1 8 19 4 32 31 1 12 4 32 31 1 9 3 113 7 168 97 7 36 1 8 104 36 1 12 92 2 35 34 105 95 2 27 94 4 28 4 11 30 1 9 18 20 16 17 1 7 3 203 123 2 62 36 1 21 2 54 3 100 4 74 4 7 33 1 8 58 7 83 1 39 16 27 55 4 70 4 35 34 1 18 2 36 1 19 2 43 10 85 1 61 16 170 323 141 131 108 35 34 1 12 65 36 1 19 37 11 7 90 1 39 16 20 16 10 23 14 10 82 1 39 16 101 1 9 59 2 27 55 4 71 4 3 118 149 244 182 11 30 1 12 63 10 82 1 61 16 7 90 1 13 20 29 1 8 66 37 35 34 1 8 19 27 100 120 148 97 29 1 10 81 69 62 1 12 66 2 107 126 28 76 4 191 96 13 14 1 11 30 1 69 21 35 34 1 58 2 135 4 70 4 72 7 33 1 9 60 7 33 1 8 19 4 32 31 1 21 7 80 1 39 13 33 1 21 15 25 29 1 195 26 98 29 1 12 60 2 4 32 31 1 59 20 16 10 23 14 35 34 1 12 60 27 55 4 74 28 3 145 115 163 110 29 1 12 67 2 62 1 8 57 37 35 34 1 8 66 29 1 9 58 15 29 1 9 57 37 39 16 17 1 7 62 1 18 2 49 40 16 17 1 10 11 30 1 8 63 3 55 91 7 147 29 1 8 21 2 11 30 1 9 58 99 124 10 38 4 99 127 4 74 4 29 1 12 18 2 35 34 1 9 66 29 1 9 67 37 27 55 4 73 4 20 16 1 13 14 10 38 1 20 16 99 124 4 72 4 20 16 1 13 14 3 55 91 10 38 101 1 9 18 2 10 85 1 61 16 20 109 10 23 14 29 1 21 2 45 27 145 121 148 97 3 55 4 78 4 3 55 4 75 4 101 1 58 69 2 11 30 1 12 59 7 36 1 10 81 3 55 4 78 4 36 1 19 37 11 11 30 1 9 125 191 96 13 14 1 11 30 1 8 7 3 100 4 72 10 7 80 1 20 16 4 32 31 1 9 27 55 4 74 10 39 13 14 1 10 11 30 1 8 65 27 143 121 151 97 3 158 91 7 147 146 10 218 1 40 10 220 1 20 109 29 1 12 19 15 7 33 1 9 18 101 1 9 65 15 35 34 1 9 66 62 1 8 60 2 7 33 1 12 21 10 82 1 20 16 1 3 223 15 35 4 32 31 1 9 20 16 17 1 7 11 30 1 59 15 3 158 91 144 163 7 86 1 64 16 35 34 1 19 15 11 30 1 104 15 20 16 10 23 14 62 105 103 9 19 3 113 120 168 97 64 16 17 1 10 35 34 1 9 66 33 1 66 37 11 4 32 31 1 12 29 1 9 59 2 29 1 8 60 15 35 34 1 12 59 36 1 9 7 102 7 33 1 8 58 62 1 8 57 2 35 34 1 9 18 62 1 8 104 15 107 126 10 38 4 36 1 9 59 37 7 33 1 10 81 35 34 1 60 2 7 83 1 10 16 84 1 20 16 17 99 127 91 28 76 7 36 1 8 10 29 1 8 10 81 4 32 31 1 9 101 1 8 18 2 7 33 1 9 60 99 127 91 4 77 39 16 10 23 14 7 33 1 60 26 11 30 1 104 26 29 1 59 26 98 3 55 4 73 4 36 1 19 2 52 29 1 12 59 2 7 89 1 20 16 10 85 1 20 16 29 1 57 15 25 106 130 4 74 6 101 1 8 18 37 4 32 31 1 9 20 16 1 13 14 99 127 10 38 4 99 127 4 75 28 40 16 17 13 1 29 1 8 18 37 4 32 31 1 12 36 1 104 2 45 116 100 4 78 4 3 94 4 71 4 3 55 91 4 77 11 30 1 243 15 33 105 103 9 19 7 89 1 39 13 7 80 1 20 13 4 32 31 1 19 35 34 1 58 8 4 32 31 1 9 35 34 1 9 18 3 55 10 38 28 4 32 31 1 9 3 118 113 7 97 64 16 1 13 14 4 32 31 1 9 3 55 91 28 76 11 30 1 12 92 27 143 7 151 97 62 1 9 63 2 35 34 1 9 57 20 16 17 1 13 36 1 12 18 2 11 30 1 12 19 212 99 124 278 0 7 87 1 20 16 3 94 4 70 4 135 4 74 4 73 11 30 1 9 19 27 94 4 28 4 29 1 9 26 63 7 90 1 20 16 36 1 9 125 2 7 90 1 64 16 7 33 1 104 2 27 55 7 288 153 11 30 1 9 21 20 16 17 1 10 11 30 1 104 2 7 62 1 10 81 3 55 4 70 6 27 113 120 148 97 106 130 28 76 4 20 16 10 23 14 4 32 31 1 9 27 55 4 77 4 29 1 69 21 2 116 100 4 74 4 29 1 9 66 15 129 96 10 23 14 29 1 12 66 15 146 7 201 1 39 4 32 31 1 9 27 55 4 74 4 29 1 8 112 15 3 160 171 1 198 27 55 4 77 78 3 100 4 75 28 11 30 1 9 59 106 130 28 76 4 4 32 31 1 8 11 30 1 7 102 27 94 10 38 4 116 100 38 4 71 20 16 1 13 14 36 1 8 63 37 84 1 20 16 10 7 90 1 61 16 29 1 68 2 52 27 55 4 70 4 11 30 1 18 2 29 1 9 19 2 36 105 103 9 19 20 16 10 23 14 135 4 73 4 28 29 1 9 58 2 35 34 1 8 21 29 1 8 63 2 36 1 63 2 52 29 1 68 15 25 7 80 1 64 16 7 33 1 12 59 116 207 22 136 7 64 16 17 1 10 3 55 115 248 110 35 34 1 9 117 35 34 1 19 2 27 94 4 75 6 3 94 121 147 97 36 1 8 18 2 29 1 9 138 2 99 131 233 2 36 11 30 1 8 59 101 1 9 65 2 35 34 105 103 9 29 1 9 18 26 61 16 17 13 1 3 100 4 74 4 4 32 31 1 12 62 1 8 92 2 27 55 4 78 4 135 4 71 6 4 7 33 1 12 63 7 33 1 8 10 29 1 9 22 102 20 16 1 13 14 11 30 1 12 68 129 96 13 14 1 135 10 38 4 77 3 55 4 28 4 35 34 1 8 21 11 30 1 12 68 99 124 10 38 6 29 1 9 58 37 29 1 8 19 2 7 62 1 9 10 36 1 58 26 98 40 16 10 23 14 27 143 146 165 97 7 86 1 61 16 11 30 1 8 21 115 216 122 20 109 107 126 4 77 4 107 126 4 78 4 3 143 10 248 110 11 30 1 10 81 35 34 1 9 65 27 1 10 344 22 62 1 9 19 2 10 82 1 64 16 35 34 1 10 81 27 178 22 167 181 4 32 31 1 21 27 55 4 71 4 3 55 91 10 38 20 16 17 1 10 7 33 1 9 58 263 179 209 141 175 4 32 31 1 7 7 33 1 12 18 62 1 69 21 15 36 1 9 117 2 4 32 31 1 8 64 16 17 1 84 20 16 17 13 1 99 124 4 72 4 35 34 1 21 2 7 36 1 8 306 4 32 31 1 21 40 16 1 13 14 35 34 1 12 60 7 33 1 9 18 35 34 1 12 63 33 1 57 2 43 29 1 12 59 2 20 16 10 23 14 135 4 71 4 72 39 16 17 1 7 7 33 1 9 19 11 30 1 8 18 7 89 1 20 16 62 1 12 63 2 35 34 1 63 37 64 16 17 1 13 29 1 9 125 2 29 1 8 18 2 27 145 120 148 97 146 11 219 122 20 99 124 4 71 4 35 34 1 7 102 29 1 8 112 37 3 94 4 71 10 106 130 4 73 4 27 55 28 76 4 4 32 31 1 12 33 1 21 2 54 27 158 91 144 183 3 55 4 70 4 3 94 4 70 4 29 1 9 66 2 11 30 1 12 60 3 118 113 144 200 99 124 4 78 4 40 16 17 13 1 62 1 8 104 26 20 16 17 1 7 27 158 91 7 151 36 1 8 66 2 7 33 1 10 81 39 16 17 1 7 4 32 31 1 9 101 1 8 18 15 7 33 1 8 63 107 132 4 71 6 11 30 1 9 22 27 55 121 148 97 4 32 31 1 12 10 85 1 64 16 35 34 1 9 65 7 33 1 9 19 35 34 1 18 2 27 55 120 97 155 4 32 31 1 12 4 32 31 1 8 29 1 12 92 2 4 32 31 1 12 61 16 17 1 10 4 32 31 1 9 263 179 209 141 4 7 33 1 8 58 36 1 8 137 2 106 128 91 4 28 40 16 10 23 14 62 1 8 67 2 7 33 1 8 92 4 32 31 1 9 29 1 8 18 15 7 36 1 8 92 11 30 1 12 18 11 30 1 19 2 106 130 4 73 4 235 1 115 290 180 35 34 1 10 81 36 1 9 10 81 61 16 1 13 14 62 1 9 7 102 101 1 9 195 37 40 13 14 1 10 29 1 12 67 15 35 34 1 8 67 20 16 1 13 14 3 55 4 70 4 7 33 1 9 104 27 55 121 151 97 7 33 1 12 68 62 1 8 60 2 7 33 1 9 104 4 32 31 1 10 10 38 1 64 16 106 130 4 73 4 129 96 13 14 1 4 32 31 1 8 106 130 4 28 4 84 1 13 39 16 27 204 123 15 35 27 118 149 121 97 7 62 1 10 81 20 16 1 13 14 7 201 1 20 109 29 1 21 15 25 35 34 1 60 15 11 30 1 63 15 3 55 91 4 73 40 16 10 23 14 29 1 8 18 15 10 85 1 20 16 4 32 31 1 8 11 30 1 9 60 61 13 14 16 1 106 128 91 4 78 7 90 1 20 16 36 1 9 138 2 64 16 17 1 13 7 86 1 20 16 4 32 31 1 58 29 1 8 21 15 35 34 1 12 63 29 1 9 67 15 101 1 8 67 2 20 16 17 13 1 61 16 17 1 7 35 34 1 8 63 35 34 1 9 58 3 113 121 168 97 11 30 1 8 59 3 100 4 75 4 7 87 1 64 16 7 80 1 40 16 36 1 7 102 15 29 1 12 18 2 29 1 12 63 37 29 1 60 15 111 62 1 8 19 15 116 160 171 1 196 4 32 31 1 8 36 1 8 67 2 27 55 4 71 4 36 1 9 22 102 36 1 12 67 2 3 55 146 11 193 3 55 91 28 76 7 90 1 61 16 29 1 8 19 2 7 33 1 8 18 4 32 31 1 12 170 141 131 108 258 107 126 4 28 4 107 132 4 28 4 39 16 10 23 14 36 1 60 15 25 3 113 121 151 97 4 32 31 1 10 29 1 9 104 37 29 1 58 69 15 4 32 31 1 9 62 1 8 92 2 101 1 19 37 11 20 109 10 23 14 7 86 1 64 16 11 30 1 9 67 7 33 1 8 60 7 87 1 10 16 39 16 17 1 13 3 55 10 38 4 4 32 31 1 9 35 34 1 10 81 99 124 4 71 4 7 89 1 20 16 62 1 57 2 48 7 33 1 9 65 62 105 103 9 19 99 124 4 70 4 40 109 10 23 14 7 36 1 12 59 27 55 4 70 10 35 34 1 12 18 4 32 31 1 12 106 130 4 73 6 62 1 12 19 2 3 55 4 73 4 3 100 4 75 28 101 1 60 15 25 135 4 74 4 78 33 1 59 15 25 29 1 65 37 11 33 105 103 9 19 29 1 9 65 37 107 208 120 254 2 7 86 1 20 16 152 96 10 23 14 152 96 10 23 14 39 16 1 13 14 11 30 1 8 18 10 38 1 20 13 40 16 10 23 14 107 132 4 78 4 7 36 1 8 57 27 118 113 121 168 27 118 113 121 168 62 1 12 57 2 11 30 1 69 65 29 1 8 19 15 3 55 91 4 75 7 33 1 8 58 11 30 1 249 15 35 34 1 12 19 7 88 1 61 16 29 1 66 2 49 99 127 4 73 4 33 1 58 37 11 27 158 91 7 151 7 90 1 20 16 205 229 99 10 277 20 16 17 1 7 7 33 1 12 63 62 1 9 19 2 10 85 1 39 16 7 33 1 12 67 7 80 1 39 16 10 85 1 61 16 7 90 1 13 39 29 1 19 2 49 20 16 17 13 1 40 16 10 23 14 29 1 12 19 2 84 1 13 39 16 27 55 4 28 6 29 1 8 66 15 3 143 120 97 134 36 1 8 63 37 116 100 91 4 71 62 1 12 18 2 7 33 1 104 15 62 1 8 59 2 7 33 1 8 104 35 34 1 8 18 7 33 1 12 92 106 130 4 74 6 4 32 31 1 8 4 32 31 1 112 27 94 4 73 4 29 1 8 18 2 99 240 123 2 36 39 16 17 1 84 7 62 1 8 10 29 1 12 68 15 101 1 58 69 26 20 109 10 23 156 27 94 115 110 182 7 62 1 9 58 35 34 1 9 10 7 87 1 61 16 35 34 1 10 81 62 1 63 15 25 135 4 73 4 159 11 30 1 10 81 7 87 1 7 13 116 100 91 4 75 27 55 4 77 4 106 128 91 4 77 7 33 1 8 18 35 34 1 8 66 7 86 1 61 16 20 16 17 1 13 4 32 31 1 8 62 1 8 21 2 3 158 91 144 110 3 178 22 136 29 10 82 1 40 16 40 16 17 1 13 62 1 9 104 26 29 1 12 63 2 152 96 10 23 14 39 109 17 1 13 7 87 1 10 16 4 32 31 1 8 11 30 1 12 19 62 1 66 2 50 4 32 31 1 67 27 55 10 184 110 7 33 1 12 67 7 87 1 40 13 20 16 1 13 14 3 94 4 28 4 27 1 234 2 48 7 33 1 8 59 35 34 1 10 81 11 30 1 12 60 11 30 1 9 26 61 16 17 1 7 152 96 10 23 14 101 1 12 59 2 29 1 66 15 42 101 1 9 60 2 7 33 1 12 59 29 1 21 26 98 7 33 1 8 18 29 1 9 22 102 64 16 17 1 7 27 55 4 77 4 3 55 91 6 28 35 34 1 9 60 29 1 12 65 2 4 32 31 1 65 7 33 1 12 19 3 158 91 121 97 4 32 31 1 9 10 85 1 39 16 20 109 10 23 14 4 32 31 1 9 3 94 4 28 28 64 16 17 1 7 10 82 1 20 16 7 62 1 9 10 106 130 4 74 4 29 1 8 19 2 29 1 8 21 15 4 32 31 1 18 135 4 74 4 71 35 34 1 8 66 20 16 10 23 14 7 33 1 8 21 33 1 59 2 51 29 1 10 81 69 35 34 1 18 26 33 1 63 2 48 10 85 1 61 16 99 127 38 28 76 29 1 67 37 11 29 1 8 63 37 7 36 1 58 69 152 96 10 23 14 27 55 4 73 4 29 1 9 59 26 40 16 17 1 7 29 1 9 65 2 64 16 17 1 13 4 32 31 1 19 3 160 171 1 198 11 30 1 10 81 7 33 1 9 68 235 294 116 26 29 7 33 1 12 68 27 160 146 303 197 99 124 4 77 4 29 1 8 92 2 62 1 65 2 51 11 30 1 9 104 101 1 9 59 37 7 36 1 10 81 61 13 14 16 1 11 30 1 9 60 11 30 1 8 137 36 1 9 58 37 10 38 1 64 16 33 1 67 15 42 10 38 1 20 16 27 55 10 38 4 27 145 120 151 97 3 55 4 70 4 11 30 1 8 18 129 96 10 23 14 61 16 17 1 13 36 1 21 15 25 29 1 8 92 2 20 16 17 13 1 27 55 4 70 4 35 34 1 9 117 36 1 21 15 25 7 33 1 9 58 40 16 17 1 10 4 32 31 1 9 152 96 10 23 14 27 94 4 70 6 35 34 1 12 67 7 86 1 20 13 11 30 1 8 57 4 257 131 170 141 7 90 1 39 13 10 85 1 7 13 4 32 31 1 58 7 33 1 12 18 116 160 146 7 194 99 127 4 71 28 115 217 122 40 109 27 94 10 110 133 36 1 9 10 81 20 16 17 1 7 10 82 1 39 16 29 1 8 18 2 29 1 12 65 2 20 16 17 1 7 61 16 17 1 7 36 1 8 67 2 7 33 1 60 2 29 1 9 10 81 35 34 1 9 63 27 55 120 147 97 3 100 4 72 4 4 32 31 1 249 11 30 1 249 15 7 80 1 20 16 36 1 9 22 225 29 1 63 37 11 20 16 17 1 13 11 30 1 69 65 29 1 8 57 2 27 94 4 28 28 3 55 4 28 4 11 30 1 9 67 4 32 31 1 12 62 1 8 63 2 27 55 91 28 76 35 34 1 57 2 4 32 31 105 131 99 124 4 28 4 3 55 91 120 168 62 1 8 68 2 27 94 4 28 38 27 94 4 78 4 29 1 9 21 15 35 34 1 104 2 7 87 1 40 16 39 13 14 108 96 27 55 7 168 97 20 16 1 13 14 101 1 9 60 2 35 34 1 12 57 4 32 31 1 12 7 89 1 64 16 106 130 10 38 6 27 180 173 32 31 205 131 99 91 123 35 34 1 8 65 7 36 1 8 19 101 1 60 15 25 29 1 8 65 37 33 1 66 37 11 29 1 8 18 2 20 16 17 13 1 255 122 40 254 26 101 1 8 67 15 29 1 12 19 15 11 30 1 9 66 7 33 1 8 63 27 143 120 148 97 36 1 9 66 15 20 16 17 13 1 27 94 4 71 4 116 100 91 4 77 7 33 1 8 92 3 94 4 71 4 35 34 1 12 18 35 34 1 19 15 4 32 31 1 58 3 143 115 184 110 106 130 4 70 4 11 30 1 9 117 62 1 63 37 11 3 94 4 28 4 106 128 91 4 72 4 32 31 1 12 84 1 64 16 17 7 80 1 20 16 11 30 1 9 7 7 86 1 64 16 27 55 4 71 4 3 55 91 10 200 4 32 31 1 12 36 1 9 60 2 10 82 1 40 16 10 82 1 39 13 29 1 12 18 15 106 130 10 38 4 11 30 1 58 2 99 124 4 73 10 7 83 1 7 13 35 34 1 9 63 29 1 8 18 2 11 30 1 12 57 20 16 10 23 14 35 34 1 8 18 7 80 1 39 13 7 33 1 9 104 11 30 1 8 18 3 143 121 147 97 3 100 4 74 4 11 30 1 12 19 7 86 1 61 16 61 16 17 1 7 36 1 8 18 2 7 33 1 8 137 3 55 4 74 4 4 32 31 1 8 3 145 120 97 134 7 36 1 58 12 35 34 1 8 19 27 94 4 28 4 7 89 1 20 16 39 16 10 23 14 38 131 191 96 4 27 247 121 147 97 4 32 31 1 8 27 94 4 75 4 62 1 21 2 49 36 1 8 66 2 35 34 1 12 92 39 16 17 1 7 11 30 1 9 104 27 55 4 74 4 4 32 31 1 60 36 1 9 22 102 27 55 4 77 6 3 55 4 78 4 3 55 4 77 4 7 36 1 8 10 11 30 1 67 2 35 34 1 9 58 129 96 10 23 14 4 32 31 1 58 20 16 17 13 1 106 128 91 4 78 84 1 40 16 13 3 100 4 73 4 3 1 198 22 164 4 32 31 1 9\n\t [[Node: loss_1/time_distributed_2_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss_1/time_distributed_2_loss/Reshape_1, loss_1/time_distributed_2_loss/Cast)]]\n"
     ]
    }
   ],
   "source": [
    "def simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a basic RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Build the layers\n",
    "    learning_rate = 0.001\n",
    "    rnn_units = french_vocab_size\n",
    "    print(rnn_units)\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(rnn_units, input_shape=input_shape[1:],\n",
    "                        activation='sigmoid', return_sequences=True))\n",
    "\n",
    "    model.add(SimpleRNN(rnn_units, input_shape=input_shape[1:],\n",
    "                        activation='sigmoid', return_sequences=True))\n",
    "    \n",
    "    model.add(SimpleRNN(rnn_units, input_shape=input_shape[1:],\n",
    "                        activation='sigmoid', return_sequences=True))\n",
    "        \n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
    "    model.compile(loss=sparse_categorical_crossentropy, optimizer=Adam(learning_rate), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "    # Alternative model using GRU\n",
    "    # trains a little better but has more trainable params in network (820,312)\n",
    "#     gru_units = 256\n",
    "#     model = Sequential()\n",
    "#     #model.add(Input(shape=input_shape[1:]))\n",
    "#     model.add(GRU(units=gru_units, input_shape=input_shape[1:], return_sequences=True, activation=None))\n",
    "#     model.add(PReLU())\n",
    "\n",
    "#     # Each of the `output_sequence_length` output time steps should\n",
    "#     # be a vector of `max(1024, 2*french_vocab_size)` dimensions.\n",
    "#     model.add(TimeDistributed(Dense(max(1024, 2*french_vocab_size), activation=None)))\n",
    "#     model.add(PReLU())\n",
    "    \n",
    "#     # Not applying dropout because the network doesn't appear to\n",
    "#     # overfit in the given scenario.\n",
    "        \n",
    "#     # Each of the `output_sequence_length` output time steps should\n",
    "#     # be a vector of `french_vocab_size` dimensions.\n",
    "#     model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
    "#     model.compile(loss=sparse_categorical_crossentropy,\n",
    "#                   optimizer=Adam(learning_rate),\n",
    "#                   metrics=['accuracy'])\n",
    "#     return model\n",
    "    \n",
    "    \n",
    "tests.test_simple_model(simple_model)\n",
    "\n",
    "# Reshaping the input to work with a basic RNN\n",
    "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "# Train the neural network\n",
    "simple_rnn_model = simple_model(\n",
    "    tmp_x.shape,\n",
    "    max_french_sequence_length,\n",
    "    english_vocab_size,\n",
    "    french_vocab_size)\n",
    "simple_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Print prediction(s)\n",
    "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Embedding (IMPLEMENTATION)\n",
    "![RNN](images/embedding.png)\n",
    "You've turned the words into ids, but there's a better representation of a word.  This is called word embeddings.  An embedding is a vector representation of the word that is close to similar words in n-dimensional space, where the n represents the size of the embedding vectors.\n",
    "\n",
    "In this model, you'll create a RNN model using embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 5, 227)            45173     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 5, 227)            413140    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 5, 227)            413140    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 5, 344)            78432     \n",
      "=================================================================\n",
      "Total params: 949,885\n",
      "Trainable params: 949,885\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[478,0] = 199 is not in [0, 199)\n\t [[Node: embedding_2/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_2/embeddings/read, embedding_2/Cast)]]\n\nCaused by op 'embedding_2/Gather', defined at:\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/asyncio/base_events.py\", line 1425, in _run_once\n    handle._run()\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/asyncio/events.py\", line 126, in _run\n    self._callback(*self._args)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-17-659d9745695e>\", line 59, in <module>\n    len(french_tokenizer.word_index))\n  File \"<ipython-input-17-659d9745695e>\", line 18, in embed_model\n    model.add(Embedding(english_vocab_size, max_vocab_size, input_length=input_shape[1]))\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/keras/models.py\", line 467, in add\n    layer(x)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/keras/engine/topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/keras/layers/embeddings.py\", line 138, in call\n    out = K.gather(self.embeddings, inputs)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 1211, in gather\n    return tf.gather(reference, indices)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 2698, in gather\n    params, indices, validate_indices=validate_indices, name=name)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2672, in gather\n    validate_indices=validate_indices, name=name)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3290, in create_op\n    op_def=op_def)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1654, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): indices[478,0] = 199 is not in [0, 199)\n\t [[Node: embedding_2/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_2/embeddings/read, embedding_2/Cast)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    517\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[478,0] = 199 is not in [0, 199)\n\t [[Node: embedding_2/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_2/embeddings/read, embedding_2/Cast)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-659d9745695e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0membedding_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreproc_french_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# TODO: Print prediction(s)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[478,0] = 199 is not in [0, 199)\n\t [[Node: embedding_2/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_2/embeddings/read, embedding_2/Cast)]]\n\nCaused by op 'embedding_2/Gather', defined at:\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/asyncio/base_events.py\", line 1425, in _run_once\n    handle._run()\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/asyncio/events.py\", line 126, in _run\n    self._callback(*self._args)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-17-659d9745695e>\", line 59, in <module>\n    len(french_tokenizer.word_index))\n  File \"<ipython-input-17-659d9745695e>\", line 18, in embed_model\n    model.add(Embedding(english_vocab_size, max_vocab_size, input_length=input_shape[1]))\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/keras/models.py\", line 467, in add\n    layer(x)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/keras/engine/topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/keras/layers/embeddings.py\", line 138, in call\n    out = K.gather(self.embeddings, inputs)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 1211, in gather\n    return tf.gather(reference, indices)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 2698, in gather\n    params, indices, validate_indices=validate_indices, name=name)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2672, in gather\n    validate_indices=validate_indices, name=name)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3290, in create_op\n    op_def=op_def)\n  File \"/Users/calvinoh/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1654, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): indices[478,0] = 199 is not in [0, 199)\n\t [[Node: embedding_2/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_2/embeddings/read, embedding_2/Cast)]]\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a RNN model using word embedding on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    learning_rate = 0.001\n",
    "    rnn_units = french_vocab_size\n",
    "    max_vocab_size = len(english_words_counter)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(english_vocab_size, max_vocab_size, input_length=input_shape[1]))\n",
    "    model.add(LSTM(max_vocab_size, return_sequences=True))\n",
    "    model.add(LSTM(max_vocab_size, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
    "    model.compile(loss=sparse_categorical_crossentropy, optimizer=Adam(learning_rate), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#     gru_units = 256\n",
    "#     english_embedding_size = 256\n",
    "#     inputs = Input(shape=input_shape[1:])\n",
    "    \n",
    "#     embed_english = Embedding(english_vocab_size, english_embedding_size, input_length=input_shape[1])(inputs)\n",
    "    \n",
    "#     recurrent = GRU(units=gru_units, return_sequences=True, activation=None)(embed_english)\n",
    "#     recurrent = PReLU()(recurrent)\n",
    "\n",
    "#     dense = TimeDistributed(Dense(max(1024, 2*french_vocab_size), activation=None))(recurrent)\n",
    "#     dense = PReLU()(dense)\n",
    "    \n",
    "#     output = TimeDistributed(Dense(french_vocab_size, activation='softmax'))(dense)\n",
    "    \n",
    "#     model = Model(inputs=inputs, outputs=output)\n",
    "#     model.compile(loss=sparse_categorical_crossentropy,\n",
    "#                   optimizer=Adam(learning_rate),\n",
    "#                   metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "    \n",
    "    return model\n",
    "    \n",
    "tests.test_embed_model(embed_model)\n",
    "\n",
    "\n",
    "# TODO: Reshape the input\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "\n",
    "# TODO: Train the neural network\n",
    "embedding_model = embed_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_french_sentences.shape[1],\n",
    "    len(english_tokenizer.word_index),\n",
    "    len(french_tokenizer.word_index))\n",
    "\n",
    "print(embedding_model.summary())\n",
    "\n",
    "embedding_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=100, validation_split=0.2)\n",
    "\n",
    "# TODO: Print prediction(s)\n",
    "print(logits_to_text(embedding_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Bidirectional RNNs (IMPLEMENTATION)\n",
    "![RNN](images/bidirectional.png)\n",
    "One restriction of a RNN is that it can't see the future input, only the past.  This is where bidirectional recurrent neural networks come in.  They are able to see the future data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "def bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a bidirectional RNN model on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    learning_rate = 0.001\n",
    "    gru_units = french_vocab_size\n",
    "    max_vocab_size = len(english_words_counter)\n",
    "    \n",
    "    print('input_shape', input_shape)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(english_vocab_size, max_vocab_size, input_length=input_shape[1]))\n",
    "    model.add(Bidirectional(GRU(units=gru_units, return_sequences=True, activation=None), \n",
    "                            input_shape=input_shape[1:]))\n",
    "    model.add(PReLU())\n",
    "    model.add(Bidirectional(GRU(units=gru_units, return_sequences=True, activation=None), \n",
    "                        input_shape=input_shape[1:]))\n",
    "    model.add(TimeDistributed(Dense(max(1024, 2*french_vocab_size), activation=None)))\n",
    "    model.add(PReLU())\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#tests.test_bd_model(bd_model)\n",
    "\n",
    "\n",
    "# TODO: Train and Print prediction(s)\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "#tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "# Train the neural network\n",
    "bidirectional_model = bd_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_french_sentences.shape[1],\n",
    "    len(english_tokenizer.word_index),\n",
    "    len(french_tokenizer.word_index))\n",
    "\n",
    "print(bidirectional_model.summary())\n",
    "\n",
    "bidirectional_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=100, validation_split=0.2)\n",
    "\n",
    "# Print prediction(s)\n",
    "print(logits_to_text(bidirectional_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: Encoder-Decoder (OPTIONAL)\n",
    "Time to look at encoder-decoder models.  This model is made up of an encoder and decoder. The encoder creates a matrix representation of the sentence.  The decoder takes this matrix as input and predicts the translation as output.\n",
    "\n",
    "Create an encoder-decoder model in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encdec_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train an encoder-decoder model on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # OPTIONAL: Implement\n",
    "    return None\n",
    "tests.test_encdec_model(encdec_model)\n",
    "\n",
    "\n",
    "# OPTIONAL: Train and Print prediction(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Custom (IMPLEMENTATION)\n",
    "Use everything you learned from the previous models to create a model that incorporates embedding and a bidirectional rnn into one model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "def model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a model that incorporates embedding, encoder-decoder, and bidirectional RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    learning_rate = 0.001\n",
    "    gru_units = french_vocab_size\n",
    "    max_vocab_size = len(english_words_counter)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(english_vocab_size, max_vocab_size, input_length=input_shape[1]))\n",
    "    model.add(LSTM(max_vocab_size, return_sequences=True))\n",
    "    model.add(Bidirectional(GRU(units=gru_units, return_sequences=True, activation=None), \n",
    "                            input_shape=input_shape[1:]))\n",
    "\n",
    "    model.add(Bidirectional(GRU(units=gru_units, return_sequences=True, activation=None), \n",
    "                            input_shape=input_shape[1:]))\n",
    "    \n",
    "#     model.add(TimeDistributed(Dense(max(1024, 2*max_vocab_size), activation=None)))\n",
    "#     model.add(PReLU())\n",
    "    \n",
    "    # encoder layer\n",
    "    model.add(GRU(units=gru_units, return_sequences=False, activation=None))\n",
    "    model.add(PReLU())\n",
    "\n",
    "    \n",
    "    # Get the last output of the GRU and repeats it, decoder\n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    \n",
    "    # decoder layer\n",
    "    model.add(GRU(units=gru_units, return_sequences=True, activation=None))\n",
    "    model.add(PReLU())\n",
    "    \n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
    "    \n",
    "    model.compile(loss=sparse_categorical_crossentropy, optimizer=Adam(learning_rate), metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#tests.test_model_final(model_final)\n",
    "\n",
    "\n",
    "print('Final Model Loaded')\n",
    "# TODO: Train the final model\n",
    "\n",
    "#tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "#tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "# Train the neural network\n",
    "# final_model = model_final(\n",
    "#     tmp_x.shape,\n",
    "#     preproc_french_sentences.shape[1],\n",
    "#     len(english_tokenizer.word_index),\n",
    "#     len(french_tokenizer.word_index))\n",
    "\n",
    "#print(final_model.summary())\n",
    "\n",
    "#final_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=100, validation_split=0.2)\n",
    "\n",
    "# Print prediction(s)\n",
    "#print(logits_to_text(final_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction (IMPLEMENTATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "def final_predictions(x, y, x_tk, y_tk):\n",
    "    \"\"\"\n",
    "    Gets predictions using the final model\n",
    "    :param x: Preprocessed English data\n",
    "    :param y: Preprocessed French data\n",
    "    :param x_tk: English tokenizer\n",
    "    :param y_tk: French tokenizer\n",
    "    \"\"\"\n",
    "    # TODO: Train neural network using model_final\n",
    "    model = model_final(x.shape, y.shape[1], len(x_tk.word_index), len(y_tk.word_index))\n",
    "    print(model.summary())\n",
    "    model.fit(x, preproc_french_sentences, batch_size=1024, epochs=35, validation_split=0.2)\n",
    "    \n",
    "    ## DON'T EDIT ANYTHING BELOW THIS LINE\n",
    "    y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n",
    "    y_id_to_word[0] = '<PAD>'\n",
    "\n",
    "    sentence = 'he saw a old yellow truck'\n",
    "    sentence = [x_tk.word_index[word] for word in sentence.split()]\n",
    "    sentence = pad_sequences([sentence], maxlen=x.shape[-1], padding='post')\n",
    "    sentences = np.array([sentence[0], x[0]])\n",
    "    predictions = model.predict(sentences, len(sentences))\n",
    "\n",
    "    print('Sample 1:')\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]))\n",
    "    print('Il a vu un vieux camion jaune')\n",
    "    print('Sample 2:')\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[1]]))\n",
    "    print(' '.join([y_id_to_word[np.max(x)] for x in y[0]]))\n",
    "\n",
    "\n",
    "final_predictions(preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "When you're ready to submit, complete the following steps:\n",
    "1. Review the [rubric](https://review.udacity.com/#!/rubrics/1004/view) to ensure your submission meets all requirements to pass\n",
    "2. Generate an HTML version of this notebook\n",
    "\n",
    "  - Run the next cell to attempt automatic generation (this is the recommended method in Workspaces)\n",
    "  - Navigate to **FILE -> Download as -> HTML (.html)**\n",
    "  - Manually generate a copy using `nbconvert` from your shell terminal\n",
    "```\n",
    "$ pip install nbconvert\n",
    "$ python -m nbconvert machine_translation.ipynb\n",
    "```\n",
    "  \n",
    "3. Submit the project\n",
    "\n",
    "  - If you are in a Workspace, simply click the \"Submit Project\" button (bottom towards the right)\n",
    "  \n",
    "  - Otherwise, add the following files into a zip archive and submit them \n",
    "  - `helper.py`\n",
    "  - `machine_translation.ipynb`\n",
    "  - `machine_translation.html`\n",
    "    - You can export the notebook by navigating to **File -> Download as -> HTML (.html)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!!python -m nbconvert *.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Enhancements\n",
    "\n",
    "This project focuses on learning various network architectures for machine translation, but we don't evaluate the models according to best practices by splitting the data into separate test & training sets -- so the model accuracy is overstated. Use the [`sklearn.model_selection.train_test_split()`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function to create separate training & test datasets, then retrain each of the models using only the training set and evaluate the prediction accuracy using the hold out test set. Does the \"best\" model change?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
